{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1.MNIST数据集的定义及载入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as nf\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "#device\n",
    "device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "train_dir = '../processed_data/train/'\n",
    "val_dir = '../processed_data/val/'\n",
    "test_dir = '../processed_data/test/'\n",
    "# 对数据进行归一化\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
    "\n",
    "class MNIST_Dataset(Dataset):\n",
    "    def __init__(self, root_dir,setname,transform = None):\n",
    "        self.setname = setname\n",
    "        self.root_dir = root_dir\n",
    "        self.subfolders = sorted(os.listdir(root_dir))\n",
    "        self.data = []\n",
    "        self.labels = []\n",
    "        self.transform = transform\n",
    "        if (self.setname=='train') or (self.setname=='val'):\n",
    "            for label, subfolder in enumerate(self.subfolders):\n",
    "                folder_path = os.path.join(root_dir, subfolder)\n",
    "                file_list = sorted(os.listdir(folder_path))\n",
    "\n",
    "                for file_name in file_list:\n",
    "                    file_path = os.path.join(folder_path, file_name)\n",
    "                    data = np.load(file_path)\n",
    "                    ## 只取data第一个方向的第一个维度,因为其他方向的数据全是0\n",
    "                    data = data[0,:,:]\n",
    "                    ## 对数据进行归一化\n",
    "                    # mean_value = np.mean(data)\n",
    "                    # std_value = np.std(data)\n",
    "                    # if std_value == 0:\n",
    "                    #     data = data / 255.0\n",
    "                    # else:\n",
    "                    #     data = (data - mean_value) / std_value\n",
    "                    ## 将.npy数据转化为torch类型的数据\n",
    "                    # data = torch.from_numpy(data)\n",
    "                    self.data.append(data)\n",
    "                    self.labels.append(label)\n",
    "        else: #测试集数据\n",
    "            file_list = sorted(os.listdir(root_dir))\n",
    "            for file_name in file_list:\n",
    "                file_path = os.path.join(root_dir, file_name)\n",
    "                data = np.load(file_path)\n",
    "                ## 只取data第一个方向的第一个维度,因为其他方向的数据全是0\n",
    "                data = data[0,:,:]\n",
    "                ## 对数据进行归一化\n",
    "                # mean_value = np.mean(data)\n",
    "                # std_value = np.std(data)\n",
    "                # if std_value == 0:\n",
    "                #     data = data / 255.0\n",
    "                # else:\n",
    "                #     data = (data - mean_value) / std_value\n",
    "                ## 将.npy数据转化为torch类型的数据\n",
    "                # data = torch.from_numpy(data)\n",
    "                self.data.append(data)\n",
    "                # self.labels.append(label) 测试数据无标签\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if (self.setname=='train') or (self.setname=='val'):\n",
    "            data_pair = {\n",
    "                'data': self.data[idx],\n",
    "                'label': self.labels[idx]\n",
    "            }\n",
    "            if self.transform:\n",
    "                data_pair['data'] = self.transform(data_pair['data'])\n",
    "        else:\n",
    "            data_pair = {\n",
    "                'data': self.data[idx]\n",
    "            }\n",
    "            if self.transform:\n",
    "                data_pair['data'] = self.transform(data_pair['data'])\n",
    "        return data_pair\n",
    "\n",
    "\n",
    "#不用transform里面的均值和方差了,自己求均值方差归一化数据试一试\n",
    "train_dataset = MNIST_Dataset(train_dir,\"train\", transform = transform)\n",
    "val_dataset = MNIST_Dataset(val_dir,\"val\", transform = transform)\n",
    "test_dataset = MNIST_Dataset(test_dir,\"test\", transform = transform)\n",
    "\n",
    "\n",
    "#分别创建两个DataLoader载入训练集与测试集的数据\n",
    "# 注意batch-size表示每批样本的大小，一次训练迭代一个batch.因此len(data_train_loader)表示mini-batch的数目\n",
    "#batch_idx表示batch批的数目下标\n",
    "data_train_loader = DataLoader(train_dataset, batch_size=512 ,shuffle = True, num_workers=0,drop_last = False)  # 训练集的数据被随机打乱\n",
    "data_val_loader = DataLoader(val_dataset, batch_size=10 ,shuffle = False, num_workers=0,drop_last = False)  # 训练集的数据被随机打乱\n",
    "data_test_loader = DataLoader(test_dataset, batch_size=1024 ,shuffle = False, num_workers=0,drop_last = False)  # 训练集的数据被随机打乱\n",
    "num_data_val = len(val_dataset)\n",
    "num_batch_val = np.ceil(num_data_val / data_val_loader.batch_size)\n",
    "# print(num_batch_val)\n",
    "# print(len(train_dataset))\n",
    "# print(len(val_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part2.MNIST数据可视化并和正确的数据集图片作对比"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ../real_data/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../real_data/MNIST/raw/train-images-idx3-ubyte.gz to ../real_data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ../real_data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../real_data/MNIST/raw/train-labels-idx1-ubyte.gz to ../real_data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ../real_data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../real_data/MNIST/raw/t10k-images-idx3-ubyte.gz to ../real_data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ../real_data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n",
      "Extracting ../real_data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ../real_data/MNIST/raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAesAAAGzCAYAAAAPLj87AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8g+/7EAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAxXUlEQVR4nO3de1hVdd7//xeQbDQBQ+KkeDYtDzCpEFlqIyOaWU5WZt1q3mUn6KtSZjYqWt0xY2WWN2U1mXVNptmkHSy6DUMvR9TyMI73baRoIx7A0whKCQ6s3x/93LUTlLXZuNdyPR/Xta4r1l7vvT57zZ5evT9r7bUCDMMwBAAALCvQ3wMAAADnRlgDAGBxhDUAABZHWAMAYHGENQAAFkdYAwBgcYQ1AAAWR1gDAGBxhDUAABZHWAMN0K5dO91zzz3+HgaAixxhDdShqKhIDzzwgDp06KCQkBCFhYWpb9++eumll/Tjjz/6e3hu+/fv1x133KEWLVooLCxMt9xyi3bv3u3vYQHwoUv8PQDAilasWKHbb79dLpdLY8aMUffu3VVVVaW1a9dq8uTJ+t///V+9/vrr/h6mTp48qRtuuEFlZWV68skn1aRJE7344ovq37+/tm7dqpYtW/p7iAB8gLAGfmXPnj2688471bZtW61atUqxsbHu19LT07Vr1y6tWLHCjyP82SuvvKKdO3dq48aN6tOnjyRpyJAh6t69u1544QU9++yzfh4hAF9gGhz4ldmzZ+vkyZN68803PYL6jE6dOmnChAm11h47dkyPPfaYevTooebNmyssLExDhgzR3//+97O2nTdvnrp166ZmzZrpsssuU+/evbVo0SL36ydOnNDEiRPVrl07uVwuRUVF6Xe/+502b97s3uaDDz5Qnz593EEtSV27dtXAgQP1/vvvN+QwALAQOmvgVz755BN16NBB1157rena3bt3a/ny5br99tvVvn17lZaW6rXXXlP//v31f//3f4qLi5MkvfHGG/p//+//6bbbbtOECRN06tQpbdu2TRs2bNBdd90lSXrwwQf1wQcfKCMjQ1dddZWOHj2qtWvXaseOHbr66qtVU1Ojbdu26T//8z/PGkdSUpL+53/+RydOnFBoaGjDDggAvyOsgV8oLy/X/v37dcstt3hV36NHD3333XcKDPx50mr06NHq2rWr3nzzTU2fPl3ST+fEu3XrpqVLl9b5XitWrND48eP1wgsvuNc9/vjj7n8+duyYKisra+3+z6w7cOCAunTp4tVnAWAdTIMDv1BeXi5JXnejLpfLHdTV1dU6evSomjdvri5dunhMX7do0UL79u3T119/Xed7tWjRQhs2bNCBAwdqff3MFekul+us10JCQjy2AWBvhDXwC2FhYZJ+Ol/sjZqaGr344ovq3LmzXC6XIiMjdfnll2vbtm0qKytzbzdlyhQ1b95cSUlJ6ty5s9LT0/W3v/3N471mz56t7du3Kz4+XklJSZo5c6bHT7KaNm0qSaqsrDxrHKdOnfLYBoC9EdbAL4SFhSkuLk7bt2/3qv7ZZ59VZmam+vXrp7/85S/64osvtHLlSnXr1k01NTXu7a688koVFhZq8eLFuu666/TXv/5V1113nbKystzb3HHHHdq9e7fmzZunuLg4Pffcc+rWrZs+//xzSVJERIRcLpcOHjx41jjOrDtzjhyAvQUYhmH4exCAlTzwwAN6/fXXtW7dOqWkpJxz23bt2mnAgAFauHChJCkxMVERERFatWqVx3atW7dWp06dlJ+fX+v7VFVV6dZbb1Vubq5Onjzpnsb+pUOHDunqq69Wu3bttHbtWklSnz59FBAQoI0bN3psO2jQIBUVFamoqKienxqAldFZA7/y+OOP69JLL9V9992n0tLSs14vKirSSy+9VGttUFCQfv3fv0uXLtX+/fs91h09etTj7+DgYF111VUyDEOnT59WdXW1x7S5JEVFRSkuLs5j2vu2227T119/rW+++ca9rrCwUKtWrdLtt99evw8MwPK4Ghz4lY4dO2rRokUaOXKkrrzySo87mK1bt05Lly6t837gN910k5566imNGzdO1157rf7xj3/o3XffVYcOHTy2GzRokGJiYtS3b19FR0drx44d+u///m8NHTpUoaGhOn78uFq3bq3bbrtNCQkJat68ub788kt9/fXXHleHP/zww3rjjTc0dOhQPfbYY2rSpInmzJmj6OhoPfroo415mABcSAaAWn333XfG+PHjjXbt2hnBwcFGaGio0bdvX2PevHnGqVOnDMMwjLZt2xpjx45115w6dcp49NFHjdjYWKNp06ZG3759jYKCAqN///5G//793du99tprRr9+/YyWLVsaLpfL6NixozF58mSjrKzMMAzDqKysNCZPnmwkJCQYoaGhxqWXXmokJCQYr7zyylnjLC4uNm677TYjLCzMaN68uXHTTTcZO3fubNRjA+DC4pw1AAAWxzlrAAAsjrAGAMDiCGsAACyOsAYAwIQ1a9Zo2LBhiouLU0BAgJYvX37emvz8fF199dVyuVzq1KmT+94M9UVYAwBgQkVFhRISEpSTk1Ov7ffs2aOhQ4fqhhtu0NatWzVx4kTdd999+uKLL+q9T64GBwDASwEBAVq2bJmGDx9e5zZTpkzRihUrPG5jfOedd+r48ePKzc2t134sd1OUmpoaHThwQKGhoQoICPD3cAAAJhmGoRMnTiguLs7jcbG+durUKVVVVTX4fQzDOCtvXC5XrU+080ZBQYFSU1M91qWlpWnixIn1fg/LhfWBAwcUHx/v72EAABqouLhYrVu3bpT3PnXqlNq3b6+SkpIGv1fz5s118uRJj3VZWVmaOXNmg99bkkpKShQdHe2xLjo6WuXl5frxxx/r9XQ8y4W1t88RBgBYS2P++7yqqkolJSXau3ev+9G23igvL1ebNm1UXFzs8T6+6qp9pdHCOicnR88995xKSkqUkJCgefPmKSkp6bx1TH0DwMXhQvz7PCwsrEFh7ev3qU1MTMxZDwUqLS1VWFhYvZ853ygnE5YsWaLMzExlZWVp8+bNSkhIUFpamg4dOtQYuwMAOJRhGA1eGltKSory8vI81q1cufK8j+D10Bg3HE9KSjLS09Pdf1dXVxtxcXFGdnb2eWvLysoMSSwsLCwsNl/OPJimMZzJimPHjhn//ve/vV6OHTtmeqwnTpwwtmzZYmzZssWQZMyZM8fYsmWL8c9//tMwDMN44oknjNGjR7u33717t9GsWTNj8uTJxo4dO4ycnBwjKCjIyM3Nrfc+fR7WlZWVRlBQkLFs2TKP9WPGjDFuvvnms7Y/deqUUVZW5l6Ki4v9/gVjYWFhYWn4ciHC+ujRo8bp06e9Xo4ePWp6rF999VWtn/fME/jGjh3r8ZS9MzWJiYlGcHCw0aFDB+Ott94y9Xl9fs76yJEjqq6urvXKt2+//fas7bOzszVr1ixfDwMAgEYxYMCAc06f13Z3sgEDBmjLli1e79PvdzCbOnWqysrK3EtxcbG/hwQAsAnDBuesfcHnnXVkZKSCgoJqvfItJibmrO19+cNzAICzNDRw7RLWPu+sg4OD1atXL48r32pqapSXl2fuyjcAACCpkX5nnZmZqbFjx6p3795KSkrS3LlzVVFRoXHjxjXG7gAADuWUzrpRwnrkyJE6fPiwZsyYoZKSEiUmJio3N/esi84AAGgIp4S15Z66VV5ervDwcH8PAwDQQGVlZY12V7AzWXHmTmANeZ/o6OhGHasvWO7e4AAA1JdTOmvCGgBgW04Ja7//zhoAAJwbnTUAwLac0lkT1gAA2yKsAQCwOKeENeesAQCwODprAIBtOaWzJqwBALbllLBmGhwAAIujswYA2JZTOmvCGgBgW04Ja6bBAQCwODprAIBtOaWzJqwBALZml8BtCKbBAQCwODprAIBtMQ0OAIDFEdYAAFicU8Kac9YAAFgcnTUAwLac0lkT1gAA23JKWDMNDgCAxdFZAwBsyymdNWENALAtp4Q10+AAAFgcnTUAwLac0lkT1gAA23JKWDMNDgCAxdFZAwBsyymdNWENALAtwhoAAItzSlhzzhoAAIujswYA2JZTOmvCGgBgW04Ja6bBAQCwODprAIBtOaWzJqwBALbllLBmGhwAAIujswZsolevXqZrMjIyvNrXmDFjTNe88847pmvmzZtnumbz5s2ma3DxckpnTVgDAGzNLoHbEEyDAwBgcXTWAADbYhocAACLI6wBALA4p4Q156wBALA4OmsAgG05pbMmrAEAtuWUsGYaHAAAi6OzBgDYllM6a8IaAGBbTglrpsEBALA4OmvADxITE03XrFy50nRNWFiY6RrJu25j9OjRpmtuvvlm0zUtW7Y0XYOLl1M6a8IaAGBbTglrpsEBALA4n4f1zJkzFRAQ4LF07drV17sBAMDdWTdksYNGmQbv1q2bvvzyy593cgmz7QAA32MavAEuueQSxcTEuJfIyMjG2A0AwOH81Vnn5OSoXbt2CgkJUXJysjZu3HjO7efOnasuXbqoadOmio+P16RJk3Tq1Kl6769Rwnrnzp2Ki4tThw4ddPfdd2vv3r11bltZWany8nKPBQAAq1qyZIkyMzOVlZWlzZs3KyEhQWlpaTp06FCt2y9atEhPPPGEsrKytGPHDr355ptasmSJnnzyyXrv0+dhnZycrIULFyo3N1evvvqq9uzZo+uvv14nTpyodfvs7GyFh4e7l/j4eF8PCQBwkfJHZz1nzhyNHz9e48aN01VXXaX58+erWbNmWrBgQa3br1u3Tn379tVdd92ldu3aadCgQRo1atR5u/Ff8nlYDxkyRLfffrt69uyptLQ0ffbZZzp+/Ljef//9WrefOnWqysrK3EtxcbGvhwQAuEj5Kqx/PcNbWVlZ6/6qqqq0adMmpaamutcFBgYqNTVVBQUFtdZce+212rRpkzucd+/erc8++0w33nhjvT9no1/51aJFC11xxRXatWtXra+7XC65XK7GHgYAAHX69axuVlaWZs6cedZ2R44cUXV1taKjoz3WR0dH69tvv631ve+66y4dOXJE1113nQzD0L///W89+OCD/p0G/7WTJ0+qqKhIsbGxjb0rAIDD+KqzLi4u9pjlnTp1qs/GmJ+fr2effVavvPKKNm/erA8//FArVqzQ008/Xe/38Hln/dhjj2nYsGFq27atDhw4oKysLAUFBWnUqFG+3hUAwOF89dOtsLCwet2eNzIyUkFBQSotLfVYX1paqpiYmFprpk+frtGjR+u+++6TJPXo0UMVFRW6//779Yc//EGBgefvm33eWe/bt0+jRo1Sly5ddMcdd6hly5Zav369Lr/8cl/vCgCACyo4OFi9evVSXl6ee11NTY3y8vKUkpJSa80PP/xwViAHBQVJqv/vvH3eWS9evNjXbwlYWlJSkumav/71r6ZrwsPDTdd423HU9euNc6mqqjJd481DOa655hrTNZs3bzZdI3n3mXBh+eOmKJmZmRo7dqx69+6tpKQkzZ07VxUVFRo3bpwkacyYMWrVqpWys7MlScOGDdOcOXP0m9/8RsnJydq1a5emT5+uYcOGuUP7fLi1GADA1i70XchGjhypw4cPa8aMGSopKVFiYqJyc3PdF53t3bvXo5OeNm2aAgICNG3aNO3fv1+XX365hg0bpv/6r/+q9z4JawAATMrIyFBGRkatr+Xn53v8fckllygrK0tZWVle74+wBgDYllPuDU5YAwBsi7AGAMDinBLWjX5TFAAA0DB01gAA23JKZ01YAwBsyylhzTQ4AAAWR2cNALAtp3TWhDUAwLacEtZMgwMAYHF01rgoNWvWzKu6q6++2nTNX/7yF9M1Vn+++86dO03XzJ4923SNNw/++dvf/ma6Ztq0aaZrJLkfxADrckpnTVgDAGzLKWHNNDgAABZHZw0AsC2ndNaENQDAtghrAAAszilhzTlrAAAsjs4aAGBbTumsCWsAgG05JayZBgcAwOLorAEAtuWUzpqwBgDYllPCmmlwAAAsjs4aAGBbTumsCWtclF577TWv6kaNGuXjkdiTN08fa968uema1atXm64ZMGCA6ZqePXuaroF92CVwG4JpcAAALI7OGgBgW0yDAwBgcYQ1AAAW55Sw5pw1AAAWR2cNALAtp3TWhDUAwLacEtZMgwMAYHF01gAA23JKZ01YAwBsyylhzTQ4AAAWR2cNALAtp3TWhDUsr1evXqZrhg4d6tW+AgICvKozy5sHWHzyySema55//nnTNZJ04MAB0zVbtmwxXfOvf/3LdM1vf/tb0zUX6n9XXHhOCWumwQEAsDg6awCAbTmlsyasAQC2RVgDAGBxTglrzlkDAGBxdNYAANtySmdNWAMAbMspYc00OAAAFkdnDQCwLad01oQ1AMC2nBLWTIMDAGBxdNYAANtySmdNWOOCSkxMNF2zcuVK0zVhYWGmayTv/o/7+eefm64ZNWqU6Zr+/fubrpk2bZrpGkn685//bLrm8OHDpmv+/ve/m66pqakxXePtg12uvvpq0zWbN2/2al/wjlPCmmlwAAAsjs4aAGBrdumOG8J0Z71mzRoNGzZMcXFxCggI0PLlyz1eNwxDM2bMUGxsrJo2barU1FTt3LnTV+MFAMDtzDR4QxY7MB3WFRUVSkhIUE5OTq2vz549Wy+//LLmz5+vDRs26NJLL1VaWppOnTrV4MECAPBLTglr09PgQ4YM0ZAhQ2p9zTAMzZ07V9OmTdMtt9wiSXrnnXcUHR2t5cuX684772zYaAEAcCCfXmC2Z88elZSUKDU11b0uPDxcycnJKigoqLWmsrJS5eXlHgsAAPXhlM7ap2FdUlIiSYqOjvZYHx0d7X7t17KzsxUeHu5e4uPjfTkkAMBFjLC+QKZOnaqysjL3Ulxc7O8hAQBgKT796VZMTIwkqbS0VLGxse71paWldd4Mw+VyyeVy+XIYAACH4KYoXmjfvr1iYmKUl5fnXldeXq4NGzYoJSXFl7sCAMAx0+CmO+uTJ09q165d7r/37NmjrVu3KiIiQm3atNHEiRP1zDPPqHPnzmrfvr2mT5+uuLg4DR8+3JfjBgDAMUyH9TfffKMbbrjB/XdmZqYkaezYsVq4cKEef/xxVVRU6P7779fx48d13XXXKTc3VyEhIb4bNQAAcs40uOmwHjBgwDk/XEBAgJ566ik99dRTDRoYrO+KK64wXTN58mTTNeHh4aZrjhw5YrpGkg4ePGi65u233zZdc/LkSdM1K1asuCA1F6OmTZt6Vffoo4+arrn77ru92he846+wzsnJ0XPPPaeSkhIlJCRo3rx5SkpKqnP748eP6w9/+IM+/PBDHTt2TG3bttXcuXN144031mt/3BscAGBb/gjrJUuWKDMzU/Pnz1dycrLmzp2rtLQ0FRYWKioq6qztq6qq9Lvf/U5RUVH64IMP1KpVK/3zn/9UixYt6r1PwhoAABPmzJmj8ePHa9y4cZKk+fPna8WKFVqwYIGeeOKJs7ZfsGCBjh07pnXr1qlJkyaSpHbt2pnap99/Zw0AgLd8dTX4r++kWVlZWev+qqqqtGnTJo87dQYGBio1NbXOO3V+/PHHSklJUXp6uqKjo9W9e3c9++yzqq6urvfnJKwBALblq7COj4/3uJtmdnZ2rfs7cuSIqqurTd2pc/fu3frggw9UXV2tzz77TNOnT9cLL7ygZ555pt6fk2lwAIDjFRcXKywszP23L2/WVVNTo6ioKL3++usKCgpSr169tH//fj333HPKysqq13sQ1gAA2/LVBWZhYWEeYV2XyMhIBQUFqbS01GN9aWmp+y6evxYbG6smTZooKCjIve7KK69USUmJqqqqFBwcfN79Mg0OALCtC30Hs+DgYPXq1cvjTp01NTXKy8ur806dffv21a5du1RTU+Ne99133yk2NrZeQS0R1gAAmJKZmak33nhDb7/9tnbs2KGHHnpIFRUV7qvDx4wZo6lTp7q3f+ihh3Ts2DFNmDBB3333nVasWKFnn31W6enp9d4n0+AAANvyx++sR44cqcOHD2vGjBkqKSlRYmKicnNz3Red7d27V4GBP/fC8fHx+uKLLzRp0iT17NlTrVq10oQJEzRlypR675OwBgDYlr/uYJaRkaGMjIxaX8vPzz9rXUpKitavX+/VviSmwQEAsDw6awCAbfEgDwAALI6whmN4++P/559/3nRNfZ8w80snTpwwXTNmzBjTNdJPj4A1y9snOsH62rRp4+8hoB7sErgNwTlrAAAsjs4aAGBbTIMDAGBxTglrpsEBALA4OmsAgG05pbMmrAEAtuWUsGYaHAAAi6OzBgDYllM6a8IaAGBbTglrpsEBALA4OmsAgG05pbMmrAEAtkVYwzF+85vfeFXnzUM5vHHLLbeYrlm9enUjjASA1TglrDlnDQCAxdFZAwBsyymdNWENALAtp4Q10+AAAFgcnTUAwLac0lkT1gAA23JKWDMNDgCAxdFZAwBsyymdNWENALAtp4Q10+AAAFgcnTUAwLac0lkT1gAA2yKs4Rhz5szxqi4gIMB0jTcP2OChHPilwEDzZ+9qamoaYSSwCrsEbkNwzhoAAIujswYA2BbT4AAAWJxTwpppcAAALI7OGgBgW07prAlrAIBtOSWsmQYHAMDi6KwBALbllM6asAYA2JZTwpppcAAALI7OGgBgW07prAlrAIBtEdawpZtuusl0TWJiolf78uZL/vHHH3u1L+AMbx7K4e2/kLdu3epVHS4cp4Q156wBALA4OmsAgG3RWddhzZo1GjZsmOLi4hQQEKDly5d7vH7PPfcoICDAYxk8eLCvxgsAgNuZsG7IYgemw7qiokIJCQnKycmpc5vBgwfr4MGD7uW9995r0CABAHAy09PgQ4YM0ZAhQ865jcvlUkxMjNeDAgCgPpgGb4D8/HxFRUWpS5cueuihh3T06NE6t62srFR5ebnHAgBAfTAN7qXBgwfrnXfeUV5env70pz9p9erVGjJkiKqrq2vdPjs7W+Hh4e4lPj7e10MCAMDWfH41+J133un+5x49eqhnz57q2LGj8vPzNXDgwLO2nzp1qjIzM91/l5eXE9gAgHphGtxHOnTooMjISO3atavW110ul8LCwjwWAADqg2lwH9m3b5+OHj2q2NjYxt4VAAAXJdPT4CdPnvTokvfs2aOtW7cqIiJCERERmjVrlkaMGKGYmBgVFRXp8ccfV6dOnZSWlubTgQMA4JRpcNNh/c033+iGG25w/33mfPPYsWP16quvatu2bXr77bd1/PhxxcXFadCgQXr66aflcrl8N2oAAERY12nAgAHn/HBffPFFgwaEhmnatKnpmuDgYK/2dejQIdM1S5Ys8WpfsD5v/oN85syZvh9ILVatWuVV3dSpU308EjQGuwRuQ/AgDwAALI4HeQAAbItpcAAALM4pYc00OAAAFkdnDQCwLad01oQ1AMC2nBLWTIMDAGBxhDUAwLb8dW/wnJwctWvXTiEhIUpOTtbGjRvrVbd48WIFBARo+PDhpvZHWAMAbMsfYb1kyRJlZmYqKytLmzdvVkJCgtLS0s57o6jvv/9ejz32mK6//nrT+ySsAQCOV15e7rFUVlbWue2cOXM0fvx4jRs3TldddZXmz5+vZs2aacGCBXXWVFdX6+6779asWbPUoUMH0+MjrAEAtuWrzjo+Pl7h4eHuJTs7u9b9VVVVadOmTUpNTXWvCwwMVGpqqgoKCuoc51NPPaWoqCjde++9Xn1OrgYHANiWr64GLy4uVlhYmHt9Xfe6P3LkiKqrqxUdHe2xPjo6Wt9++22tNWvXrtWbb76prVu3ej1OwhoAYFu+CuuwsDCPsPaVEydOaPTo0XrjjTcUGRnp9fsQ1vDauc7p1OXgwYONMBL4mjdP0Jo2bZrpmsmTJ5uu2bdvn+maF154wXSNJJ08edKrOly8IiMjFRQUpNLSUo/1paWliomJOWv7oqIiff/99xo2bJh7XU1NjSTpkksuUWFhoTp27Hje/XLOGgBgWxf6avDg4GD16tVLeXl57nU1NTXKy8tTSkrKWdt37dpV//jHP7R161b3cvPNN+uGG27Q1q1bFR8fX6/90lkDAGzLH3cwy8zM1NixY9W7d28lJSVp7ty5qqio0Lhx4yRJY8aMUatWrZSdna2QkBB1797do75FixaSdNb6cyGsAQAwYeTIkTp8+LBmzJihkpISJSYmKjc3133R2d69exUY6NuJa8IaAGBb/ro3eEZGhjIyMmp9LT8//5y1CxcuNL0/whoAYFs8yAMAAFgCnTUAwLac0lkT1gAA23JKWDMNDgCAxdFZAwBsyymdNWENALAtwhoAABuwS+A2BGENr3388cf+HgLOIzEx0as6bx6wMXLkSNM1H330kemaESNGmK4B7I6wBgDYFtPgAABYnFPCmp9uAQBgcXTWAADbckpnTVgDAGzLKWHNNDgAABZHZw0AsC2ndNaENQDAtpwS1kyDAwBgcXTWAADbckpnTVgDAGyLsAYAwOIIa9hSQEDABamRpOHDh5uumTBhglf7gjRp0iTTNdOnT/dqX+Hh4aZr3n33XdM1Y8aMMV0DOBFhDQCwLTprAAAszilhzU+3AACwODprAIBtOaWzJqwBALbllLBmGhwAAIujswYA2JZTOmvCGgBgW04Ja6bBAQCwODprAIBtOaWzJqwBALZFWAMAYHGENWzJmy+et1/WmJgY0zUvv/yy6ZoFCxaYrjl69KjpGkm65pprTNeMHj3adE1CQoLpmtatW5uu2bt3r+kaSfriiy9M17zyyite7QvA+RHWAABbs0t33BCENQDAtpwyDW7qp1vZ2dnq06ePQkNDFRUVpeHDh6uwsNBjm1OnTik9PV0tW7ZU8+bNNWLECJWWlvp00AAAOImpsF69erXS09O1fv16rVy5UqdPn9agQYNUUVHh3mbSpEn65JNPtHTpUq1evVoHDhzQrbfe6vOBAwBwprNuyGIHpqbBc3NzPf5euHChoqKitGnTJvXr109lZWV68803tWjRIv32t7+VJL311lu68sortX79eq8u3gEAoC5Mg9dDWVmZJCkiIkKStGnTJp0+fVqpqanubbp27ao2bdqooKCg1veorKxUeXm5xwIAAH7mdVjX1NRo4sSJ6tu3r7p37y5JKikpUXBwsFq0aOGxbXR0tEpKSmp9n+zsbIWHh7uX+Ph4b4cEAHAYp0yDex3W6enp2r59uxYvXtygAUydOlVlZWXupbi4uEHvBwBwDqeEtVc/3crIyNCnn36qNWvWeNyoISYmRlVVVTp+/LhHd11aWlrnDTRcLpdcLpc3wwAAwBFMddaGYSgjI0PLli3TqlWr1L59e4/Xe/XqpSZNmigvL8+9rrCwUHv37lVKSopvRgwAwP+PzroW6enpWrRokT766COFhoa6z0OHh4eradOmCg8P17333qvMzExFREQoLCxMjzzyiFJSUrgSHADgc065GtxUWL/66quSpAEDBnisf+utt3TPPfdIkl588UUFBgZqxIgRqqysVFpaGvcMBgA0CsK6FvX5UCEhIcrJyVFOTo7Xg4I9BAUFma55+OGHTdeMGDHCdI23PwHs3LmzV3UXwrp160zXfPXVV17ta8aMGV7VAWgc3BscAGBbdNYAAFicU8K6QXcwAwAAjY/OGgBgW07prAlrAIBtOSWsmQYHAMDi6KwBALbllM6asAYA2JZTwpppcAAALI7OGgBgW07prAlrAIBtEdYAAFicU8Kac9YAAFgcnfVFpqCgwHTN119/7dW++vTp41WdWTExMaZroqOjG2EktTt69KjpmsWLF5uumTBhgukawAns0h03BGENALAtpsEBAECtcnJy1K5dO4WEhCg5OVkbN26sc9s33nhD119/vS677DJddtllSk1NPef2tSGsAQC2daazbshi1pIlS5SZmamsrCxt3rxZCQkJSktL06FDh2rdPj8/X6NGjdJXX32lgoICxcfHa9CgQdq/f3+990lYAwBsy1dhXV5e7rFUVlbWuc85c+Zo/PjxGjdunK666irNnz9fzZo104IFC2rd/t1339XDDz+sxMREde3aVX/+859VU1OjvLy8en9OwhoA4Hjx8fEKDw93L9nZ2bVuV1VVpU2bNik1NdW9LjAwUKmpqfW+wPeHH37Q6dOnFRERUe/xcYEZAMC2fHWBWXFxscLCwtzrXS5XrdsfOXJE1dXVZ/3iJDo6Wt9++2299jllyhTFxcV5BP75ENYAANvyVViHhYV5hHVj+eMf/6jFixcrPz9fISEh9a4jrAEAqKfIyEgFBQWptLTUY31pael57wnx/PPP649//KO+/PJL9ezZ09R+OWcNALCtC301eHBwsHr16uVxcdiZi8VSUlLqrJs9e7aefvpp5ebmqnfv3qY/J501AMC2/HFTlMzMTI0dO1a9e/dWUlKS5s6dq4qKCo0bN06SNGbMGLVq1cp9kdqf/vQnzZgxQ4sWLVK7du1UUlIiSWrevLmaN29er30S1gAA2/JHWI8cOVKHDx/WjBkzVFJSosTEROXm5rovOtu7d68CA3+euH711VdVVVWl2267zeN9srKyNHPmzHrtk7AGAMCkjIwMZWRk1Ppafn6+x9/ff/99g/dHWF9k9u3bZ7rm1ltv9WpfDzzwgOmaadOmebWvC+Wll14yXfPqq6+artm1a5fpGgBnc8q9wQlrAIBtOSWsuRocAACLo7MGANiWUzprwhoAYFtOCWumwQEAsDg6awCAbTmlsyasAQC25ZSwZhocAACLo7MGANiWUzprwhoAYFuENQAAFueUsOacNQAAFhdgWOw/K8rLyxUeHu7vYQAAGqisrExhYWGN8t5nsqJDhw4ej6M0q6amRrt3727UsfoC0+AAANtiGhwAAFgCnTUAwLac0lkT1gAA23JKWDMNDgCAxdFZAwBsyymdNWENALAtp4Q10+AAAFgcnTUAwLac0lkT1gAA2yKsAQCwOKeENeesAQCwODprAIBtOaWzJqwBALbllLBmGhwAAIszFdbZ2dnq06ePQkNDFRUVpeHDh6uwsNBjmwEDBiggIMBjefDBB306aAAApJ8764YsdmAqrFevXq309HStX79eK1eu1OnTpzVo0CBVVFR4bDd+/HgdPHjQvcyePdungwYAQHJOWJs6Z52bm+vx98KFCxUVFaVNmzapX79+7vXNmjVTTEyMb0YIAIDDNeicdVlZmSQpIiLCY/27776ryMhIde/eXVOnTtUPP/xQ53tUVlaqvLzcYwEAoD7orM+jpqZGEydOVN++fdW9e3f3+rvuuktt27ZVXFyctm3bpilTpqiwsFAffvhhre+TnZ2tWbNmeTsMAICDOeVq8ADDy5E+9NBD+vzzz7V27Vq1bt26zu1WrVqlgQMHateuXerYseNZr1dWVqqystL9d3l5ueLj470ZEgDAQsrKyhQWFtYo711eXq7w8HBFRkYqMND7SeKamhodOXKkUcfqC1511hkZGfr000+1Zs2acwa1JCUnJ0tSnWHtcrnkcrm8GQYAwOGc0lmbCmvDMPTII49o2bJlys/PV/v27c9bs3XrVklSbGysVwMEAKAuhHUt0tPTtWjRIn300UcKDQ1VSUmJJCk8PFxNmzZVUVGRFi1apBtvvFEtW7bUtm3bNGnSJPXr1089e/ZslA8AAHAup4S1qXPWAQEBta5/6623dM8996i4uFj/8R//oe3bt6uiokLx8fH6/e9/r2nTptX7XMCZ8xAAAHu7EOesL7vssgafs/7Xv/51cZ2zPl+ux8fHa/Xq1Q0aEAAAZtilO24IHuQBALCthga1XYKeB3kAAGBxdNYAANtySmdNWAMAbMspYc00OAAAFkdnDQCwLad01oQ1AMC2nBLWTIMDAGBxdNYAANtySmdNWAMAbIuwBgDA4pwS1pyzBgDA4uisAQC25ZTOmrAGANiWU8KaaXAAACyOzhoAYFtO6awJawCAbTklrJkGBwDA4uisAQC25ZTOmrAGANiWU8KaaXAAACyOzhoAYFt01gAAWJxhGA1evJGTk6N27dopJCREycnJ2rhx4zm3X7p0qbp27aqQkBD16NFDn332man9EdYAANvyR1gvWbJEmZmZysrK0ubNm5WQkKC0tDQdOnSo1u3XrVunUaNG6d5779WWLVs0fPhwDR8+XNu3bzf1QS2lrKzMkMTCwsLCYvOlrKzsgmRFQECA14s3Y01KSjLS09Pdf1dXVxtxcXFGdnZ2rdvfcccdxtChQz3WJScnGw888EC992m5ztqwyfkDAMC5Xah/nxs+6KrLy8s9lsrKylr3VVVVpU2bNik1NdW9LjAwUKmpqSooKKi1pqCgwGN7SUpLS6tz+9pYLqxPnDjh7yEAAHygMf99HhwcrJiYGJ+8V/PmzRUfH6/w8HD3kp2dXeu2R44cUXV1taKjoz3WR0dHq6SkpNaakpISU9vXxnJXg8fFxam4uFihoaEKCAjweK28vFzx8fEqLi5WWFiYn0bofxyHn3AcfsJx+AnH4SdWOA6GYejEiROKi4trtH2EhIRoz549qqqqavB7GYZxVt64XK4Gv68vWS6sAwMD1bp163NuExYW5uj/M57BcfgJx+EnHIefcBx+4u/jEB4e3uj7CAkJUUhISKPv55ciIyMVFBSk0tJSj/WlpaV1dvoxMTGmtq+N5abBAQCwquDgYPXq1Ut5eXnudTU1NcrLy1NKSkqtNSkpKR7bS9LKlSvr3L42luusAQCwsszMTI0dO1a9e/dWUlKS5s6dq4qKCo0bN06SNGbMGLVq1cp93nvChAnq37+/XnjhBQ0dOlSLFy/WN998o9dff73e+7RVWLtcLmVlZVnuXMKFxnH4CcfhJxyHn3AcfsJxaHwjR47U4cOHNWPGDJWUlCgxMVG5ubnui8j27t2rwMCfJ66vvfZaLVq0SNOmTdOTTz6pzp07a/ny5erevXu99xlg8FspAAAsjXPWAABYHGENAIDFEdYAAFgcYQ0AgMUR1gAAWJxtwtrss0MvRjNnzlRAQIDH0rVrV38Pq9GtWbNGw4YNU1xcnAICArR8+XKP1w3D0IwZMxQbG6umTZsqNTVVO3fu9M9gG9H5jsM999xz1vdj8ODB/hlsI8nOzlafPn0UGhqqqKgoDR8+XIWFhR7bnDp1Sunp6WrZsqWaN2+uESNGnHX3KLurz3EYMGDAWd+HBx980E8jRkPZIqzNPjv0YtatWzcdPHjQvaxdu9bfQ2p0FRUVSkhIUE5OTq2vz549Wy+//LLmz5+vDRs26NJLL1VaWppOnTp1gUfauM53HCRp8ODBHt+P99577wKOsPGtXr1a6enpWr9+vVauXKnTp09r0KBBqqiocG8zadIkffLJJ1q6dKlWr16tAwcO6NZbb/XjqH2vPsdBksaPH+/xfZg9e7afRowGq/fDNP3I7LNDL1ZZWVlGQkKCv4fhV5KMZcuWuf+uqakxYmJijOeee8697vjx44bL5TLee+89P4zwwvj1cTAMwxg7dqxxyy23+GU8/nLo0CFDkrF69WrDMH76375JkybG0qVL3dvs2LHDkGQUFBT4a5iN7tfHwTAMo3///saECRP8Nyj4lOU7a2+eHXox27lzp+Li4tShQwfdfffd2rt3r7+H5Fd79uxRSUmJx/cjPDxcycnJjvx+5OfnKyoqSl26dNFDDz2ko0eP+ntIjaqsrEySFBERIUnatGmTTp8+7fF96Nq1q9q0aXNRfx9+fRzOePfddxUZGanu3btr6tSp+uGHH/wxPPiA5W83eq5nh3777bd+GpV/JCcna+HCherSpYsOHjyoWbNm6frrr9f27dsVGhrq7+H5xZnnwTb0WbEXg8GDB+vWW29V+/btVVRUpCeffFJDhgxRQUGBgoKC/D08n6upqdHEiRPVt29f920bS0pKFBwcrBYtWnhsezF/H2o7DpJ01113qW3btoqLi9O2bds0ZcoUFRYW6sMPP/TjaOEty4c1fjZkyBD3P/fs2VPJyclq27at3n//fd17771+HBms4M4773T/c48ePdSzZ0917NhR+fn5GjhwoB9H1jjS09O1fft2R1y3cS51HYf777/f/c89evRQbGysBg4cqKKiInXs2PFCDxMNZPlpcG+eHeoULVq00BVXXKFdu3b5eyh+c+Y7wPfjbB06dFBkZORF+f3IyMjQp59+qq+++kqtW7d2r4+JiVFVVZWOHz/usf3F+n2o6zjUJjk5WZIuyu+DE1g+rL15dqhTnDx5UkVFRYqNjfX3UPymffv2iomJ8fh+lJeXa8OGDY7/fuzbt09Hjx69qL4fhmEoIyNDy5Yt06pVq9S+fXuP13v16qUmTZp4fB8KCwu1d+/ei+r7cL7jUJutW7dK0kX1fXASW0yDn+/ZoU7x2GOPadiwYWrbtq0OHDigrKwsBQUFadSoUf4eWqM6efKkRzewZ88ebd26VREREWrTpo0mTpyoZ555Rp07d1b79u01ffp0xcXFafjw4f4bdCM413GIiIjQrFmzNGLECMXExKioqEiPP/64OnXqpLS0ND+O2rfS09O1aNEiffTRRwoNDXWfhw4PD1fTpk0VHh6ue++9V5mZmYqIiFBYWJgeeeQRpaSk6JprrvHz6H3nfMehqKhIixYt0o033qiWLVtq27ZtmjRpkvr166eePXv6efTwir8vR6+vefPmGW3atDGCg4ONpKQkY/369f4e0gU3cuRIIzY21ggODjZatWpljBw50ti1a5e/h9XovvrqK0PSWcvYsWMNw/jp51vTp083oqOjDZfLZQwcONAoLCz076AbwbmOww8//GAMGjTIuPzyy40mTZoYbdu2NcaPH2+UlJT4e9g+Vdvnl2S89dZb7m1+/PFH4+GHHzYuu+wyo1mzZsbvf/974+DBg/4bdCM433HYu3ev0a9fPyMiIsJwuVxGp06djMmTJxtlZWX+HTi8xvOsAQCwOMufswYAwOkIawAALI6wBgDA4ghrAAAsjrAGAMDiCGsAACyOsAYAwOIIawAALI6wBgDA4ghrAAAsjrAGAMDi/j/pePhneFqG9AAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from torchvision.datasets import MNIST\n",
    "data_real_rain = MNIST('../real_data', train = True, download=True, transform = transform)\n",
    "data_real_test = MNIST('../real_data', train=False, download=True, transform= transform)\n",
    "data_real_train_loader = DataLoader(data_real_rain, batch_size=256 ,shuffle= True, num_workers=0)  # 训练集的数据被随机打乱\n",
    "data_real_test_loader = DataLoader(data_real_test, batch_size=1024 , shuffle= False, num_workers=0) # 测试集数据不用做随机排列\n",
    "## 从训练数据集里随机抽一个MNIST数据并对其作可视化\n",
    "## 随机抽取的一张图片的路径(可以修改)\n",
    "data_path = '../processed_data/train/0/1.npy'\n",
    "class_label = data_path.split('/')[3]\n",
    "data = np.load(data_path)\n",
    "## 只取data第一个方向的第一个维度,因为其他方向的数据全是0\n",
    "data = data[0,:,:]\n",
    "## 下面进行可视化绘图\n",
    "# 使用 imshow 函数显示灰度图像\n",
    "plt.imshow(data, cmap='gray')\n",
    "# 添加标题\n",
    "plt.title('Class' + class_label)\n",
    "plt.colorbar()\n",
    "# 显示图像\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part3.构建CNN模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(1, 10, kernel_size=5),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d(kernel_size=2),\n",
    "        )\n",
    "        self.conv2 = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(10, 20, kernel_size=5),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d(kernel_size=2),\n",
    "        )\n",
    "        self.fc = torch.nn.Sequential(\n",
    "            torch.nn.Linear(320, 100),\n",
    "            torch.nn.Linear(100, 50),\n",
    "            torch.nn.Linear(50, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # print(x.shape)\n",
    "        batch_size = x.size(0)\n",
    "        # 只有在启用transform的情况下注释掉下面这行代码\n",
    "        # x = x.unsqueeze(1)\n",
    "        # print(x.shape)\n",
    "        x = self.conv1(x)  \n",
    "        x = self.conv2(x) \n",
    "        # print(x.shape)\n",
    "        x = x.view(batch_size, -1)  # flatten 变成全连接网络需要的输入 (batch, 20,4,4) ==> (batch,320), -1 此处自动算出的是320\n",
    "        # print(x.shape)\n",
    "        x = self.fc(x)\n",
    "        return x  # 最后输出的是维度为10的，也就是（对应数学符号的0~9）\n",
    "\n",
    "model = Net().to(device) # 实例化模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part4. 确定训练时需要的优化器和损失函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()  # 切换模型到训练状态\n",
    "learning_rate = 0.01\n",
    "momentum = 0.9\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum, weight_decay = 5e-4)  # lr学习率，momentum冲量\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=6, gamma=0.99, last_epoch=-1)\n",
    "writer = SummaryWriter()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part5.开始训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch: 1, batch_idx: 5]: loss: 0.023 , acc: 5.31 %\n",
      "[epoch: 1, batch_idx: 10]: loss: 0.023 , acc: 10.78 %\n",
      "[epoch: 1, batch_idx: 15]: loss: 0.023 , acc: 20.47 %\n",
      "[epoch: 1, batch_idx: 20]: loss: 0.022 , acc: 27.42 %\n",
      "[epoch: 1, batch_idx: 25]: loss: 0.022 , acc: 35.78 %\n",
      "[epoch: 1, batch_idx: 30]: loss: 0.021 , acc: 38.05 %\n",
      "[epoch: 1, batch_idx: 35]: loss: 0.018 , acc: 52.42 %\n",
      "[epoch: 1, batch_idx: 40]: loss: 0.015 , acc: 58.59 %\n",
      "[epoch: 1, batch_idx: 45]: loss: 0.009 , acc: 69.84 %\n",
      "[epoch: 1, batch_idx: 50]: loss: 0.007 , acc: 79.22 %\n",
      "[epoch: 1, batch_idx: 55]: loss: 0.005 , acc: 81.72 %\n",
      "[epoch: 1, batch_idx: 60]: loss: 0.005 , acc: 83.67 %\n",
      "[epoch: 1, batch_idx: 65]: loss: 0.004 , acc: 87.97 %\n",
      "[epoch: 1, batch_idx: 70]: loss: 0.005 , acc: 87.19 %\n",
      "[epoch: 1, batch_idx: 75]: loss: 0.005 , acc: 88.20 %\n",
      "[epoch: 1, batch_idx: 80]: loss: 0.003 , acc: 88.28 %\n",
      "[epoch: 1, batch_idx: 85]: loss: 0.004 , acc: 88.91 %\n",
      "[epoch: 1, batch_idx: 90]: loss: 0.003 , acc: 90.00 %\n",
      "[epoch: 1, batch_idx: 95]: loss: 0.003 , acc: 91.41 %\n",
      "[epoch: 1, batch_idx: 100]: loss: 0.002 , acc: 91.33 %\n",
      "[epoch: 1, batch_idx: 105]: loss: 0.002 , acc: 92.34 %\n",
      "[epoch: 1, batch_idx: 110]: loss: 0.002 , acc: 93.05 %\n",
      "[epoch: 1, batch_idx: 115]: loss: 0.002 , acc: 93.44 %\n",
      "[epoch: 1, batch_idx: 120]: loss: 0.002 , acc: 93.20 %\n",
      "[epoch: 1, batch_idx: 125]: loss: 0.002 , acc: 95.00 %\n",
      "[epoch: 1, batch_idx: 130]: loss: 0.002 , acc: 92.66 %\n",
      "[epoch: 1, batch_idx: 135]: loss: 0.003 , acc: 94.22 %\n",
      "[epoch: 1, batch_idx: 140]: loss: 0.002 , acc: 94.69 %\n",
      "[epoch: 1, batch_idx: 145]: loss: 0.002 , acc: 93.98 %\n",
      "[epoch: 1, batch_idx: 150]: loss: 0.002 , acc: 94.45 %\n",
      "[epoch: 1, batch_idx: 155]: loss: 0.002 , acc: 93.98 %\n",
      "[epoch: 1, batch_idx: 160]: loss: 0.002 , acc: 93.91 %\n",
      "[epoch: 1, batch_idx: 165]: loss: 0.002 , acc: 94.14 %\n",
      "[epoch: 1, batch_idx: 170]: loss: 0.002 , acc: 94.22 %\n",
      "[epoch: 1, batch_idx: 175]: loss: 0.002 , acc: 95.62 %\n",
      "[epoch: 1, batch_idx: 180]: loss: 0.002 , acc: 94.92 %\n",
      "[epoch: 1, batch_idx: 185]: loss: 0.001 , acc: 96.02 %\n",
      "[epoch: 1, batch_idx: 190]: loss: 0.001 , acc: 95.94 %\n",
      "[epoch: 1, batch_idx: 195]: loss: 0.001 , acc: 95.47 %\n",
      "[epoch: 1, batch_idx: 200]: loss: 0.001 , acc: 95.86 %\n",
      "[epoch: 1, batch_idx: 205]: loss: 0.002 , acc: 94.92 %\n",
      "[epoch: 1, batch_idx: 210]: loss: 0.001 , acc: 96.09 %\n",
      "[epoch: 1, batch_idx: 215]: loss: 0.001 , acc: 95.94 %\n",
      "[epoch: 1, batch_idx: 220]: loss: 0.001 , acc: 95.55 %\n",
      "[epoch: 1, batch_idx: 225]: loss: 0.001 , acc: 96.80 %\n",
      "[epoch: 1, batch_idx: 230]: loss: 0.002 , acc: 95.31 %\n",
      "[epoch: 1, batch_idx: 235]: loss: 0.001 , acc: 96.52 %\n",
      "[batch_index: 0]: Accuracy on val set: 95.7 % \n",
      "[batch_index: 1]: Accuracy on val set: 93.8 % \n",
      "[batch_index: 2]: Accuracy on val set: 95.2 % \n",
      "[batch_index: 3]: Accuracy on val set: 94.8 % \n",
      "[batch_index: 4]: Accuracy on val set: 95.9 % \n",
      "[batch_index: 5]: Accuracy on val set: 97.8 % \n",
      "[batch_index: 6]: Accuracy on val set: 97.9 % \n",
      "[batch_index: 7]: Accuracy on val set: 98.8 % \n",
      "[batch_index: 8]: Accuracy on val set: 99.0 % \n",
      "[batch_index: 9]: Accuracy on val set: 96.8 % \n",
      "Average accuracy on val set: 96.6 % \n",
      "[epoch: 2, batch_idx: 5]: loss: 0.001 , acc: 96.60 %\n",
      "[epoch: 2, batch_idx: 10]: loss: 0.001 , acc: 96.33 %\n",
      "[epoch: 2, batch_idx: 15]: loss: 0.001 , acc: 96.56 %\n",
      "[epoch: 2, batch_idx: 20]: loss: 0.001 , acc: 96.64 %\n",
      "[epoch: 2, batch_idx: 25]: loss: 0.002 , acc: 95.31 %\n",
      "[epoch: 2, batch_idx: 30]: loss: 0.002 , acc: 95.86 %\n",
      "[epoch: 2, batch_idx: 35]: loss: 0.001 , acc: 96.56 %\n",
      "[epoch: 2, batch_idx: 40]: loss: 0.001 , acc: 96.56 %\n",
      "[epoch: 2, batch_idx: 45]: loss: 0.001 , acc: 96.56 %\n",
      "[epoch: 2, batch_idx: 50]: loss: 0.001 , acc: 95.78 %\n",
      "[epoch: 2, batch_idx: 55]: loss: 0.002 , acc: 95.70 %\n",
      "[epoch: 2, batch_idx: 60]: loss: 0.001 , acc: 95.08 %\n",
      "[epoch: 2, batch_idx: 65]: loss: 0.001 , acc: 96.80 %\n",
      "[epoch: 2, batch_idx: 70]: loss: 0.001 , acc: 96.64 %\n",
      "[epoch: 2, batch_idx: 75]: loss: 0.002 , acc: 96.88 %\n",
      "[epoch: 2, batch_idx: 80]: loss: 0.001 , acc: 97.50 %\n",
      "[epoch: 2, batch_idx: 85]: loss: 0.001 , acc: 97.03 %\n",
      "[epoch: 2, batch_idx: 90]: loss: 0.002 , acc: 96.56 %\n",
      "[epoch: 2, batch_idx: 95]: loss: 0.001 , acc: 96.72 %\n",
      "[epoch: 2, batch_idx: 100]: loss: 0.001 , acc: 96.95 %\n",
      "[epoch: 2, batch_idx: 105]: loss: 0.001 , acc: 97.27 %\n",
      "[epoch: 2, batch_idx: 110]: loss: 0.001 , acc: 96.95 %\n",
      "[epoch: 2, batch_idx: 115]: loss: 0.001 , acc: 96.56 %\n",
      "[epoch: 2, batch_idx: 120]: loss: 0.001 , acc: 96.88 %\n",
      "[epoch: 2, batch_idx: 125]: loss: 0.001 , acc: 96.80 %\n",
      "[epoch: 2, batch_idx: 130]: loss: 0.001 , acc: 97.19 %\n",
      "[epoch: 2, batch_idx: 135]: loss: 0.001 , acc: 96.64 %\n",
      "[epoch: 2, batch_idx: 140]: loss: 0.001 , acc: 97.27 %\n",
      "[epoch: 2, batch_idx: 145]: loss: 0.001 , acc: 97.11 %\n",
      "[epoch: 2, batch_idx: 150]: loss: 0.001 , acc: 97.81 %\n",
      "[epoch: 2, batch_idx: 155]: loss: 0.001 , acc: 97.03 %\n",
      "[epoch: 2, batch_idx: 160]: loss: 0.001 , acc: 97.27 %\n",
      "[epoch: 2, batch_idx: 165]: loss: 0.001 , acc: 97.66 %\n",
      "[epoch: 2, batch_idx: 170]: loss: 0.001 , acc: 96.25 %\n",
      "[epoch: 2, batch_idx: 175]: loss: 0.001 , acc: 96.80 %\n",
      "[epoch: 2, batch_idx: 180]: loss: 0.001 , acc: 97.11 %\n",
      "[epoch: 2, batch_idx: 185]: loss: 0.001 , acc: 97.58 %\n",
      "[epoch: 2, batch_idx: 190]: loss: 0.001 , acc: 97.73 %\n",
      "[epoch: 2, batch_idx: 195]: loss: 0.001 , acc: 97.03 %\n",
      "[epoch: 2, batch_idx: 200]: loss: 0.001 , acc: 96.88 %\n",
      "[epoch: 2, batch_idx: 205]: loss: 0.001 , acc: 96.80 %\n",
      "[epoch: 2, batch_idx: 210]: loss: 0.001 , acc: 97.58 %\n",
      "[epoch: 2, batch_idx: 215]: loss: 0.001 , acc: 96.17 %\n",
      "[epoch: 2, batch_idx: 220]: loss: 0.001 , acc: 97.19 %\n",
      "[epoch: 2, batch_idx: 225]: loss: 0.001 , acc: 97.11 %\n",
      "[epoch: 2, batch_idx: 230]: loss: 0.002 , acc: 97.03 %\n",
      "[epoch: 2, batch_idx: 235]: loss: 0.001 , acc: 97.32 %\n",
      "[batch_index: 0]: Accuracy on val set: 97.9 % \n",
      "[batch_index: 1]: Accuracy on val set: 95.9 % \n",
      "[batch_index: 2]: Accuracy on val set: 96.7 % \n",
      "[batch_index: 3]: Accuracy on val set: 96.7 % \n",
      "[batch_index: 4]: Accuracy on val set: 97.1 % \n",
      "[batch_index: 5]: Accuracy on val set: 99.0 % \n",
      "[batch_index: 6]: Accuracy on val set: 98.4 % \n",
      "[batch_index: 7]: Accuracy on val set: 99.1 % \n",
      "[batch_index: 8]: Accuracy on val set: 99.3 % \n",
      "[batch_index: 9]: Accuracy on val set: 97.8 % \n",
      "Average accuracy on val set: 97.8 % \n",
      "[epoch: 3, batch_idx: 5]: loss: 0.001 , acc: 97.77 %\n",
      "[epoch: 3, batch_idx: 10]: loss: 0.001 , acc: 97.34 %\n",
      "[epoch: 3, batch_idx: 15]: loss: 0.001 , acc: 97.73 %\n",
      "[epoch: 3, batch_idx: 20]: loss: 0.001 , acc: 97.73 %\n",
      "[epoch: 3, batch_idx: 25]: loss: 0.001 , acc: 97.66 %\n",
      "[epoch: 3, batch_idx: 30]: loss: 0.001 , acc: 97.11 %\n",
      "[epoch: 3, batch_idx: 35]: loss: 0.001 , acc: 97.89 %\n",
      "[epoch: 3, batch_idx: 40]: loss: 0.001 , acc: 96.95 %\n",
      "[epoch: 3, batch_idx: 45]: loss: 0.001 , acc: 97.66 %\n",
      "[epoch: 3, batch_idx: 50]: loss: 0.001 , acc: 97.11 %\n",
      "[epoch: 3, batch_idx: 55]: loss: 0.001 , acc: 97.89 %\n",
      "[epoch: 3, batch_idx: 60]: loss: 0.001 , acc: 97.50 %\n",
      "[epoch: 3, batch_idx: 65]: loss: 0.001 , acc: 96.80 %\n",
      "[epoch: 3, batch_idx: 70]: loss: 0.001 , acc: 98.20 %\n",
      "[epoch: 3, batch_idx: 75]: loss: 0.001 , acc: 97.89 %\n",
      "[epoch: 3, batch_idx: 80]: loss: 0.001 , acc: 97.50 %\n",
      "[epoch: 3, batch_idx: 85]: loss: 0.001 , acc: 96.88 %\n",
      "[epoch: 3, batch_idx: 90]: loss: 0.001 , acc: 97.19 %\n",
      "[epoch: 3, batch_idx: 95]: loss: 0.001 , acc: 97.66 %\n",
      "[epoch: 3, batch_idx: 100]: loss: 0.001 , acc: 97.19 %\n",
      "[epoch: 3, batch_idx: 105]: loss: 0.001 , acc: 97.58 %\n",
      "[epoch: 3, batch_idx: 110]: loss: 0.001 , acc: 97.89 %\n",
      "[epoch: 3, batch_idx: 115]: loss: 0.001 , acc: 98.20 %\n",
      "[epoch: 3, batch_idx: 120]: loss: 0.001 , acc: 97.42 %\n",
      "[epoch: 3, batch_idx: 125]: loss: 0.001 , acc: 97.73 %\n",
      "[epoch: 3, batch_idx: 130]: loss: 0.001 , acc: 97.11 %\n",
      "[epoch: 3, batch_idx: 135]: loss: 0.001 , acc: 97.66 %\n",
      "[epoch: 3, batch_idx: 140]: loss: 0.001 , acc: 97.50 %\n",
      "[epoch: 3, batch_idx: 145]: loss: 0.001 , acc: 98.36 %\n",
      "[epoch: 3, batch_idx: 150]: loss: 0.001 , acc: 97.19 %\n",
      "[epoch: 3, batch_idx: 155]: loss: 0.002 , acc: 97.66 %\n",
      "[epoch: 3, batch_idx: 160]: loss: 0.001 , acc: 97.73 %\n",
      "[epoch: 3, batch_idx: 165]: loss: 0.001 , acc: 97.42 %\n",
      "[epoch: 3, batch_idx: 170]: loss: 0.000 , acc: 97.81 %\n",
      "[epoch: 3, batch_idx: 175]: loss: 0.000 , acc: 98.91 %\n",
      "[epoch: 3, batch_idx: 180]: loss: 0.001 , acc: 97.73 %\n",
      "[epoch: 3, batch_idx: 185]: loss: 0.001 , acc: 97.89 %\n",
      "[epoch: 3, batch_idx: 190]: loss: 0.001 , acc: 97.73 %\n",
      "[epoch: 3, batch_idx: 195]: loss: 0.001 , acc: 97.97 %\n",
      "[epoch: 3, batch_idx: 200]: loss: 0.001 , acc: 97.58 %\n",
      "[epoch: 3, batch_idx: 205]: loss: 0.000 , acc: 97.73 %\n",
      "[epoch: 3, batch_idx: 210]: loss: 0.000 , acc: 97.97 %\n",
      "[epoch: 3, batch_idx: 215]: loss: 0.001 , acc: 97.89 %\n",
      "[epoch: 3, batch_idx: 220]: loss: 0.001 , acc: 98.20 %\n",
      "[epoch: 3, batch_idx: 225]: loss: 0.001 , acc: 97.19 %\n",
      "[epoch: 3, batch_idx: 230]: loss: 0.000 , acc: 97.58 %\n",
      "[epoch: 3, batch_idx: 235]: loss: 0.002 , acc: 98.12 %\n",
      "[batch_index: 0]: Accuracy on val set: 98.1 % \n",
      "[batch_index: 1]: Accuracy on val set: 96.6 % \n",
      "[batch_index: 2]: Accuracy on val set: 97.6 % \n",
      "[batch_index: 3]: Accuracy on val set: 97.6 % \n",
      "[batch_index: 4]: Accuracy on val set: 97.8 % \n",
      "[batch_index: 5]: Accuracy on val set: 98.7 % \n",
      "[batch_index: 6]: Accuracy on val set: 98.7 % \n",
      "[batch_index: 7]: Accuracy on val set: 99.7 % \n",
      "[batch_index: 8]: Accuracy on val set: 99.4 % \n",
      "[batch_index: 9]: Accuracy on val set: 98.2 % \n",
      "Average accuracy on val set: 98.2 % \n",
      "[epoch: 4, batch_idx: 5]: loss: 0.001 , acc: 98.22 %\n",
      "[epoch: 4, batch_idx: 10]: loss: 0.001 , acc: 98.20 %\n",
      "[epoch: 4, batch_idx: 15]: loss: 0.001 , acc: 98.28 %\n",
      "[epoch: 4, batch_idx: 20]: loss: 0.000 , acc: 98.12 %\n",
      "[epoch: 4, batch_idx: 25]: loss: 0.001 , acc: 98.12 %\n",
      "[epoch: 4, batch_idx: 30]: loss: 0.000 , acc: 98.20 %\n",
      "[epoch: 4, batch_idx: 35]: loss: 0.000 , acc: 98.12 %\n",
      "[epoch: 4, batch_idx: 40]: loss: 0.000 , acc: 98.12 %\n",
      "[epoch: 4, batch_idx: 45]: loss: 0.001 , acc: 97.81 %\n",
      "[epoch: 4, batch_idx: 50]: loss: 0.001 , acc: 98.28 %\n",
      "[epoch: 4, batch_idx: 55]: loss: 0.001 , acc: 97.97 %\n",
      "[epoch: 4, batch_idx: 60]: loss: 0.001 , acc: 97.66 %\n",
      "[epoch: 4, batch_idx: 65]: loss: 0.001 , acc: 98.36 %\n",
      "[epoch: 4, batch_idx: 70]: loss: 0.001 , acc: 98.05 %\n",
      "[epoch: 4, batch_idx: 75]: loss: 0.001 , acc: 97.81 %\n",
      "[epoch: 4, batch_idx: 80]: loss: 0.000 , acc: 98.44 %\n",
      "[epoch: 4, batch_idx: 85]: loss: 0.001 , acc: 98.52 %\n",
      "[epoch: 4, batch_idx: 90]: loss: 0.001 , acc: 97.81 %\n",
      "[epoch: 4, batch_idx: 95]: loss: 0.001 , acc: 98.12 %\n",
      "[epoch: 4, batch_idx: 100]: loss: 0.000 , acc: 98.36 %\n",
      "[epoch: 4, batch_idx: 105]: loss: 0.001 , acc: 97.97 %\n",
      "[epoch: 4, batch_idx: 110]: loss: 0.001 , acc: 97.58 %\n",
      "[epoch: 4, batch_idx: 115]: loss: 0.000 , acc: 97.97 %\n",
      "[epoch: 4, batch_idx: 120]: loss: 0.001 , acc: 97.81 %\n",
      "[epoch: 4, batch_idx: 125]: loss: 0.000 , acc: 97.89 %\n",
      "[epoch: 4, batch_idx: 130]: loss: 0.000 , acc: 98.59 %\n",
      "[epoch: 4, batch_idx: 135]: loss: 0.001 , acc: 97.42 %\n",
      "[epoch: 4, batch_idx: 140]: loss: 0.001 , acc: 98.28 %\n",
      "[epoch: 4, batch_idx: 145]: loss: 0.000 , acc: 98.52 %\n",
      "[epoch: 4, batch_idx: 150]: loss: 0.000 , acc: 98.28 %\n",
      "[epoch: 4, batch_idx: 155]: loss: 0.001 , acc: 98.28 %\n",
      "[epoch: 4, batch_idx: 160]: loss: 0.001 , acc: 97.81 %\n",
      "[epoch: 4, batch_idx: 165]: loss: 0.001 , acc: 97.81 %\n",
      "[epoch: 4, batch_idx: 170]: loss: 0.001 , acc: 97.73 %\n",
      "[epoch: 4, batch_idx: 175]: loss: 0.001 , acc: 98.20 %\n",
      "[epoch: 4, batch_idx: 180]: loss: 0.001 , acc: 98.20 %\n",
      "[epoch: 4, batch_idx: 185]: loss: 0.001 , acc: 98.12 %\n",
      "[epoch: 4, batch_idx: 190]: loss: 0.000 , acc: 98.36 %\n",
      "[epoch: 4, batch_idx: 195]: loss: 0.001 , acc: 97.27 %\n",
      "[epoch: 4, batch_idx: 200]: loss: 0.001 , acc: 98.67 %\n",
      "[epoch: 4, batch_idx: 205]: loss: 0.000 , acc: 97.97 %\n",
      "[epoch: 4, batch_idx: 210]: loss: 0.000 , acc: 98.52 %\n",
      "[epoch: 4, batch_idx: 215]: loss: 0.000 , acc: 98.20 %\n",
      "[epoch: 4, batch_idx: 220]: loss: 0.000 , acc: 97.97 %\n",
      "[epoch: 4, batch_idx: 225]: loss: 0.000 , acc: 97.81 %\n",
      "[epoch: 4, batch_idx: 230]: loss: 0.000 , acc: 98.36 %\n",
      "[epoch: 4, batch_idx: 235]: loss: 0.001 , acc: 98.21 %\n",
      "[batch_index: 0]: Accuracy on val set: 97.9 % \n",
      "[batch_index: 1]: Accuracy on val set: 97.2 % \n",
      "[batch_index: 2]: Accuracy on val set: 97.9 % \n",
      "[batch_index: 3]: Accuracy on val set: 97.9 % \n",
      "[batch_index: 4]: Accuracy on val set: 97.6 % \n",
      "[batch_index: 5]: Accuracy on val set: 99.2 % \n",
      "[batch_index: 6]: Accuracy on val set: 99.0 % \n",
      "[batch_index: 7]: Accuracy on val set: 99.5 % \n",
      "[batch_index: 8]: Accuracy on val set: 99.6 % \n",
      "[batch_index: 9]: Accuracy on val set: 98.5 % \n",
      "Average accuracy on val set: 98.4 % \n",
      "[epoch: 5, batch_idx: 5]: loss: 0.002 , acc: 98.34 %\n",
      "[epoch: 5, batch_idx: 10]: loss: 0.000 , acc: 98.83 %\n",
      "[epoch: 5, batch_idx: 15]: loss: 0.001 , acc: 98.67 %\n",
      "[epoch: 5, batch_idx: 20]: loss: 0.000 , acc: 98.52 %\n",
      "[epoch: 5, batch_idx: 25]: loss: 0.001 , acc: 98.28 %\n",
      "[epoch: 5, batch_idx: 30]: loss: 0.000 , acc: 97.73 %\n",
      "[epoch: 5, batch_idx: 35]: loss: 0.000 , acc: 98.98 %\n",
      "[epoch: 5, batch_idx: 40]: loss: 0.000 , acc: 98.52 %\n",
      "[epoch: 5, batch_idx: 45]: loss: 0.001 , acc: 98.52 %\n",
      "[epoch: 5, batch_idx: 50]: loss: 0.000 , acc: 98.28 %\n",
      "[epoch: 5, batch_idx: 55]: loss: 0.001 , acc: 98.59 %\n",
      "[epoch: 5, batch_idx: 60]: loss: 0.001 , acc: 98.83 %\n",
      "[epoch: 5, batch_idx: 65]: loss: 0.001 , acc: 98.36 %\n",
      "[epoch: 5, batch_idx: 70]: loss: 0.001 , acc: 98.36 %\n",
      "[epoch: 5, batch_idx: 75]: loss: 0.000 , acc: 98.44 %\n",
      "[epoch: 5, batch_idx: 80]: loss: 0.001 , acc: 99.06 %\n",
      "[epoch: 5, batch_idx: 85]: loss: 0.000 , acc: 98.36 %\n",
      "[epoch: 5, batch_idx: 90]: loss: 0.001 , acc: 98.36 %\n",
      "[epoch: 5, batch_idx: 95]: loss: 0.001 , acc: 98.05 %\n",
      "[epoch: 5, batch_idx: 100]: loss: 0.001 , acc: 97.66 %\n",
      "[epoch: 5, batch_idx: 105]: loss: 0.001 , acc: 98.52 %\n",
      "[epoch: 5, batch_idx: 110]: loss: 0.001 , acc: 98.44 %\n",
      "[epoch: 5, batch_idx: 115]: loss: 0.000 , acc: 97.73 %\n",
      "[epoch: 5, batch_idx: 120]: loss: 0.000 , acc: 98.98 %\n",
      "[epoch: 5, batch_idx: 125]: loss: 0.001 , acc: 97.89 %\n",
      "[epoch: 5, batch_idx: 130]: loss: 0.000 , acc: 98.83 %\n",
      "[epoch: 5, batch_idx: 135]: loss: 0.001 , acc: 98.36 %\n",
      "[epoch: 5, batch_idx: 140]: loss: 0.001 , acc: 98.20 %\n",
      "[epoch: 5, batch_idx: 145]: loss: 0.001 , acc: 98.28 %\n",
      "[epoch: 5, batch_idx: 150]: loss: 0.000 , acc: 98.44 %\n",
      "[epoch: 5, batch_idx: 155]: loss: 0.001 , acc: 98.83 %\n",
      "[epoch: 5, batch_idx: 160]: loss: 0.000 , acc: 98.44 %\n",
      "[epoch: 5, batch_idx: 165]: loss: 0.001 , acc: 97.73 %\n",
      "[epoch: 5, batch_idx: 170]: loss: 0.001 , acc: 98.67 %\n",
      "[epoch: 5, batch_idx: 175]: loss: 0.001 , acc: 98.05 %\n",
      "[epoch: 5, batch_idx: 180]: loss: 0.000 , acc: 98.98 %\n",
      "[epoch: 5, batch_idx: 185]: loss: 0.000 , acc: 98.75 %\n",
      "[epoch: 5, batch_idx: 190]: loss: 0.000 , acc: 98.12 %\n",
      "[epoch: 5, batch_idx: 195]: loss: 0.001 , acc: 97.81 %\n",
      "[epoch: 5, batch_idx: 200]: loss: 0.001 , acc: 98.28 %\n",
      "[epoch: 5, batch_idx: 205]: loss: 0.000 , acc: 97.97 %\n",
      "[epoch: 5, batch_idx: 210]: loss: 0.001 , acc: 98.20 %\n",
      "[epoch: 5, batch_idx: 215]: loss: 0.000 , acc: 98.75 %\n",
      "[epoch: 5, batch_idx: 220]: loss: 0.000 , acc: 97.81 %\n",
      "[epoch: 5, batch_idx: 225]: loss: 0.001 , acc: 98.28 %\n",
      "[epoch: 5, batch_idx: 230]: loss: 0.001 , acc: 98.36 %\n",
      "[epoch: 5, batch_idx: 235]: loss: 0.001 , acc: 97.95 %\n",
      "[batch_index: 0]: Accuracy on val set: 98.3 % \n",
      "[batch_index: 1]: Accuracy on val set: 97.7 % \n",
      "[batch_index: 2]: Accuracy on val set: 98.3 % \n",
      "[batch_index: 3]: Accuracy on val set: 97.9 % \n",
      "[batch_index: 4]: Accuracy on val set: 98.0 % \n",
      "[batch_index: 5]: Accuracy on val set: 99.4 % \n",
      "[batch_index: 6]: Accuracy on val set: 98.8 % \n",
      "[batch_index: 7]: Accuracy on val set: 99.5 % \n",
      "[batch_index: 8]: Accuracy on val set: 99.5 % \n",
      "[batch_index: 9]: Accuracy on val set: 98.0 % \n",
      "Average accuracy on val set: 98.6 % \n",
      "[epoch: 6, batch_idx: 5]: loss: 0.001 , acc: 98.55 %\n",
      "[epoch: 6, batch_idx: 10]: loss: 0.001 , acc: 97.66 %\n",
      "[epoch: 6, batch_idx: 15]: loss: 0.001 , acc: 98.36 %\n",
      "[epoch: 6, batch_idx: 20]: loss: 0.001 , acc: 98.52 %\n",
      "[epoch: 6, batch_idx: 25]: loss: 0.000 , acc: 99.06 %\n",
      "[epoch: 6, batch_idx: 30]: loss: 0.000 , acc: 98.28 %\n",
      "[epoch: 6, batch_idx: 35]: loss: 0.000 , acc: 98.91 %\n",
      "[epoch: 6, batch_idx: 40]: loss: 0.002 , acc: 98.05 %\n",
      "[epoch: 6, batch_idx: 45]: loss: 0.000 , acc: 98.52 %\n",
      "[epoch: 6, batch_idx: 50]: loss: 0.000 , acc: 98.75 %\n",
      "[epoch: 6, batch_idx: 55]: loss: 0.000 , acc: 98.28 %\n",
      "[epoch: 6, batch_idx: 60]: loss: 0.000 , acc: 98.59 %\n",
      "[epoch: 6, batch_idx: 65]: loss: 0.001 , acc: 97.89 %\n",
      "[epoch: 6, batch_idx: 70]: loss: 0.001 , acc: 98.44 %\n",
      "[epoch: 6, batch_idx: 75]: loss: 0.000 , acc: 98.44 %\n",
      "[epoch: 6, batch_idx: 80]: loss: 0.000 , acc: 98.98 %\n",
      "[epoch: 6, batch_idx: 85]: loss: 0.000 , acc: 98.52 %\n",
      "[epoch: 6, batch_idx: 90]: loss: 0.001 , acc: 98.52 %\n",
      "[epoch: 6, batch_idx: 95]: loss: 0.001 , acc: 98.12 %\n",
      "[epoch: 6, batch_idx: 100]: loss: 0.001 , acc: 97.97 %\n",
      "[epoch: 6, batch_idx: 105]: loss: 0.000 , acc: 98.12 %\n",
      "[epoch: 6, batch_idx: 110]: loss: 0.000 , acc: 98.75 %\n",
      "[epoch: 6, batch_idx: 115]: loss: 0.001 , acc: 98.36 %\n",
      "[epoch: 6, batch_idx: 120]: loss: 0.000 , acc: 98.91 %\n",
      "[epoch: 6, batch_idx: 125]: loss: 0.001 , acc: 98.59 %\n",
      "[epoch: 6, batch_idx: 130]: loss: 0.000 , acc: 98.59 %\n",
      "[epoch: 6, batch_idx: 135]: loss: 0.000 , acc: 98.83 %\n",
      "[epoch: 6, batch_idx: 140]: loss: 0.000 , acc: 99.38 %\n",
      "[epoch: 6, batch_idx: 145]: loss: 0.000 , acc: 98.36 %\n",
      "[epoch: 6, batch_idx: 150]: loss: 0.001 , acc: 98.67 %\n",
      "[epoch: 6, batch_idx: 155]: loss: 0.001 , acc: 98.44 %\n",
      "[epoch: 6, batch_idx: 160]: loss: 0.000 , acc: 97.97 %\n",
      "[epoch: 6, batch_idx: 165]: loss: 0.000 , acc: 98.83 %\n",
      "[epoch: 6, batch_idx: 170]: loss: 0.000 , acc: 97.89 %\n",
      "[epoch: 6, batch_idx: 175]: loss: 0.001 , acc: 98.44 %\n",
      "[epoch: 6, batch_idx: 180]: loss: 0.000 , acc: 98.52 %\n",
      "[epoch: 6, batch_idx: 185]: loss: 0.000 , acc: 98.67 %\n",
      "[epoch: 6, batch_idx: 190]: loss: 0.001 , acc: 98.44 %\n",
      "[epoch: 6, batch_idx: 195]: loss: 0.000 , acc: 98.67 %\n",
      "[epoch: 6, batch_idx: 200]: loss: 0.000 , acc: 98.05 %\n",
      "[epoch: 6, batch_idx: 205]: loss: 0.000 , acc: 98.59 %\n",
      "[epoch: 6, batch_idx: 210]: loss: 0.000 , acc: 98.52 %\n",
      "[epoch: 6, batch_idx: 215]: loss: 0.001 , acc: 98.12 %\n",
      "[epoch: 6, batch_idx: 220]: loss: 0.000 , acc: 98.05 %\n",
      "[epoch: 6, batch_idx: 225]: loss: 0.000 , acc: 98.75 %\n",
      "[epoch: 6, batch_idx: 230]: loss: 0.000 , acc: 98.44 %\n",
      "[epoch: 6, batch_idx: 235]: loss: 0.000 , acc: 98.93 %\n",
      "[batch_index: 0]: Accuracy on val set: 98.5 % \n",
      "[batch_index: 1]: Accuracy on val set: 97.5 % \n",
      "[batch_index: 2]: Accuracy on val set: 98.6 % \n",
      "[batch_index: 3]: Accuracy on val set: 98.3 % \n",
      "[batch_index: 4]: Accuracy on val set: 98.3 % \n",
      "[batch_index: 5]: Accuracy on val set: 99.3 % \n",
      "[batch_index: 6]: Accuracy on val set: 99.3 % \n",
      "[batch_index: 7]: Accuracy on val set: 99.7 % \n",
      "[batch_index: 8]: Accuracy on val set: 99.3 % \n",
      "[batch_index: 9]: Accuracy on val set: 98.6 % \n",
      "Average accuracy on val set: 98.8 % \n",
      "[epoch: 7, batch_idx: 5]: loss: 0.001 , acc: 98.76 %\n",
      "[epoch: 7, batch_idx: 10]: loss: 0.001 , acc: 98.36 %\n",
      "[epoch: 7, batch_idx: 15]: loss: 0.000 , acc: 98.28 %\n",
      "[epoch: 7, batch_idx: 20]: loss: 0.001 , acc: 98.36 %\n",
      "[epoch: 7, batch_idx: 25]: loss: 0.000 , acc: 99.06 %\n",
      "[epoch: 7, batch_idx: 30]: loss: 0.000 , acc: 99.06 %\n",
      "[epoch: 7, batch_idx: 35]: loss: 0.001 , acc: 98.12 %\n",
      "[epoch: 7, batch_idx: 40]: loss: 0.000 , acc: 98.52 %\n",
      "[epoch: 7, batch_idx: 45]: loss: 0.000 , acc: 98.75 %\n",
      "[epoch: 7, batch_idx: 50]: loss: 0.000 , acc: 98.75 %\n",
      "[epoch: 7, batch_idx: 55]: loss: 0.000 , acc: 98.44 %\n",
      "[epoch: 7, batch_idx: 60]: loss: 0.000 , acc: 98.59 %\n",
      "[epoch: 7, batch_idx: 65]: loss: 0.001 , acc: 98.44 %\n",
      "[epoch: 7, batch_idx: 70]: loss: 0.000 , acc: 98.44 %\n",
      "[epoch: 7, batch_idx: 75]: loss: 0.000 , acc: 99.06 %\n",
      "[epoch: 7, batch_idx: 80]: loss: 0.000 , acc: 98.75 %\n",
      "[epoch: 7, batch_idx: 85]: loss: 0.001 , acc: 98.20 %\n",
      "[epoch: 7, batch_idx: 90]: loss: 0.000 , acc: 98.83 %\n",
      "[epoch: 7, batch_idx: 95]: loss: 0.001 , acc: 98.12 %\n",
      "[epoch: 7, batch_idx: 100]: loss: 0.000 , acc: 98.83 %\n",
      "[epoch: 7, batch_idx: 105]: loss: 0.001 , acc: 98.67 %\n",
      "[epoch: 7, batch_idx: 110]: loss: 0.000 , acc: 98.52 %\n",
      "[epoch: 7, batch_idx: 115]: loss: 0.000 , acc: 99.22 %\n",
      "[epoch: 7, batch_idx: 120]: loss: 0.001 , acc: 98.20 %\n",
      "[epoch: 7, batch_idx: 125]: loss: 0.000 , acc: 98.98 %\n",
      "[epoch: 7, batch_idx: 130]: loss: 0.000 , acc: 98.59 %\n",
      "[epoch: 7, batch_idx: 135]: loss: 0.000 , acc: 98.91 %\n",
      "[epoch: 7, batch_idx: 140]: loss: 0.000 , acc: 98.91 %\n",
      "[epoch: 7, batch_idx: 145]: loss: 0.001 , acc: 98.75 %\n",
      "[epoch: 7, batch_idx: 150]: loss: 0.000 , acc: 98.67 %\n",
      "[epoch: 7, batch_idx: 155]: loss: 0.001 , acc: 98.91 %\n",
      "[epoch: 7, batch_idx: 160]: loss: 0.000 , acc: 98.98 %\n",
      "[epoch: 7, batch_idx: 165]: loss: 0.001 , acc: 98.44 %\n",
      "[epoch: 7, batch_idx: 170]: loss: 0.001 , acc: 98.36 %\n",
      "[epoch: 7, batch_idx: 175]: loss: 0.000 , acc: 98.20 %\n",
      "[epoch: 7, batch_idx: 180]: loss: 0.001 , acc: 98.44 %\n",
      "[epoch: 7, batch_idx: 185]: loss: 0.001 , acc: 98.52 %\n",
      "[epoch: 7, batch_idx: 190]: loss: 0.000 , acc: 98.75 %\n",
      "[epoch: 7, batch_idx: 195]: loss: 0.001 , acc: 98.83 %\n",
      "[epoch: 7, batch_idx: 200]: loss: 0.001 , acc: 98.91 %\n",
      "[epoch: 7, batch_idx: 205]: loss: 0.000 , acc: 99.22 %\n",
      "[epoch: 7, batch_idx: 210]: loss: 0.001 , acc: 99.22 %\n",
      "[epoch: 7, batch_idx: 215]: loss: 0.000 , acc: 98.83 %\n",
      "[epoch: 7, batch_idx: 220]: loss: 0.000 , acc: 98.05 %\n",
      "[epoch: 7, batch_idx: 225]: loss: 0.000 , acc: 99.22 %\n",
      "[epoch: 7, batch_idx: 230]: loss: 0.001 , acc: 98.52 %\n",
      "[epoch: 7, batch_idx: 235]: loss: 0.001 , acc: 98.21 %\n",
      "[batch_index: 0]: Accuracy on val set: 98.1 % \n",
      "[batch_index: 1]: Accuracy on val set: 97.7 % \n",
      "[batch_index: 2]: Accuracy on val set: 98.1 % \n",
      "[batch_index: 3]: Accuracy on val set: 97.9 % \n",
      "[batch_index: 4]: Accuracy on val set: 97.9 % \n",
      "[batch_index: 5]: Accuracy on val set: 99.1 % \n",
      "[batch_index: 6]: Accuracy on val set: 99.1 % \n",
      "[batch_index: 7]: Accuracy on val set: 99.7 % \n",
      "[batch_index: 8]: Accuracy on val set: 99.7 % \n",
      "[batch_index: 9]: Accuracy on val set: 98.3 % \n",
      "Average accuracy on val set: 98.6 % \n",
      "[epoch: 8, batch_idx: 5]: loss: 0.000 , acc: 98.62 %\n",
      "[epoch: 8, batch_idx: 10]: loss: 0.000 , acc: 98.67 %\n",
      "[epoch: 8, batch_idx: 15]: loss: 0.000 , acc: 98.75 %\n",
      "[epoch: 8, batch_idx: 20]: loss: 0.001 , acc: 98.36 %\n",
      "[epoch: 8, batch_idx: 25]: loss: 0.000 , acc: 99.06 %\n",
      "[epoch: 8, batch_idx: 30]: loss: 0.000 , acc: 98.75 %\n",
      "[epoch: 8, batch_idx: 35]: loss: 0.001 , acc: 98.83 %\n",
      "[epoch: 8, batch_idx: 40]: loss: 0.000 , acc: 98.98 %\n",
      "[epoch: 8, batch_idx: 45]: loss: 0.001 , acc: 98.67 %\n",
      "[epoch: 8, batch_idx: 50]: loss: 0.000 , acc: 98.59 %\n",
      "[epoch: 8, batch_idx: 55]: loss: 0.000 , acc: 99.06 %\n",
      "[epoch: 8, batch_idx: 60]: loss: 0.000 , acc: 97.73 %\n",
      "[epoch: 8, batch_idx: 65]: loss: 0.000 , acc: 98.20 %\n",
      "[epoch: 8, batch_idx: 70]: loss: 0.001 , acc: 98.20 %\n",
      "[epoch: 8, batch_idx: 75]: loss: 0.000 , acc: 98.52 %\n",
      "[epoch: 8, batch_idx: 80]: loss: 0.000 , acc: 98.83 %\n",
      "[epoch: 8, batch_idx: 85]: loss: 0.000 , acc: 98.83 %\n",
      "[epoch: 8, batch_idx: 90]: loss: 0.000 , acc: 98.67 %\n",
      "[epoch: 8, batch_idx: 95]: loss: 0.000 , acc: 98.75 %\n",
      "[epoch: 8, batch_idx: 100]: loss: 0.001 , acc: 98.67 %\n",
      "[epoch: 8, batch_idx: 105]: loss: 0.000 , acc: 99.06 %\n",
      "[epoch: 8, batch_idx: 110]: loss: 0.000 , acc: 98.98 %\n",
      "[epoch: 8, batch_idx: 115]: loss: 0.000 , acc: 98.67 %\n",
      "[epoch: 8, batch_idx: 120]: loss: 0.000 , acc: 99.06 %\n",
      "[epoch: 8, batch_idx: 125]: loss: 0.001 , acc: 98.67 %\n",
      "[epoch: 8, batch_idx: 130]: loss: 0.000 , acc: 98.44 %\n",
      "[epoch: 8, batch_idx: 135]: loss: 0.000 , acc: 98.83 %\n",
      "[epoch: 8, batch_idx: 140]: loss: 0.000 , acc: 99.06 %\n",
      "[epoch: 8, batch_idx: 145]: loss: 0.000 , acc: 98.91 %\n",
      "[epoch: 8, batch_idx: 150]: loss: 0.000 , acc: 98.75 %\n",
      "[epoch: 8, batch_idx: 155]: loss: 0.000 , acc: 98.75 %\n",
      "[epoch: 8, batch_idx: 160]: loss: 0.000 , acc: 98.75 %\n",
      "[epoch: 8, batch_idx: 165]: loss: 0.000 , acc: 99.06 %\n",
      "[epoch: 8, batch_idx: 170]: loss: 0.000 , acc: 98.67 %\n",
      "[epoch: 8, batch_idx: 175]: loss: 0.000 , acc: 98.83 %\n",
      "[epoch: 8, batch_idx: 180]: loss: 0.000 , acc: 99.06 %\n",
      "[epoch: 8, batch_idx: 185]: loss: 0.001 , acc: 98.75 %\n",
      "[epoch: 8, batch_idx: 190]: loss: 0.000 , acc: 99.14 %\n",
      "[epoch: 8, batch_idx: 195]: loss: 0.000 , acc: 99.22 %\n",
      "[epoch: 8, batch_idx: 200]: loss: 0.000 , acc: 99.22 %\n",
      "[epoch: 8, batch_idx: 205]: loss: 0.000 , acc: 98.98 %\n",
      "[epoch: 8, batch_idx: 210]: loss: 0.001 , acc: 98.44 %\n",
      "[epoch: 8, batch_idx: 215]: loss: 0.000 , acc: 98.91 %\n",
      "[epoch: 8, batch_idx: 220]: loss: 0.001 , acc: 98.20 %\n",
      "[epoch: 8, batch_idx: 225]: loss: 0.000 , acc: 98.91 %\n",
      "[epoch: 8, batch_idx: 230]: loss: 0.001 , acc: 98.59 %\n",
      "[epoch: 8, batch_idx: 235]: loss: 0.000 , acc: 99.29 %\n",
      "[batch_index: 0]: Accuracy on val set: 98.1 % \n",
      "[batch_index: 1]: Accuracy on val set: 97.9 % \n",
      "[batch_index: 2]: Accuracy on val set: 98.0 % \n",
      "[batch_index: 3]: Accuracy on val set: 98.5 % \n",
      "[batch_index: 4]: Accuracy on val set: 98.2 % \n",
      "[batch_index: 5]: Accuracy on val set: 99.7 % \n",
      "[batch_index: 6]: Accuracy on val set: 99.3 % \n",
      "[batch_index: 7]: Accuracy on val set: 99.9 % \n",
      "[batch_index: 8]: Accuracy on val set: 99.8 % \n",
      "[batch_index: 9]: Accuracy on val set: 98.6 % \n",
      "Average accuracy on val set: 98.8 % \n",
      "[epoch: 9, batch_idx: 5]: loss: 0.001 , acc: 98.80 %\n",
      "[epoch: 9, batch_idx: 10]: loss: 0.000 , acc: 98.83 %\n",
      "[epoch: 9, batch_idx: 15]: loss: 0.000 , acc: 98.36 %\n",
      "[epoch: 9, batch_idx: 20]: loss: 0.001 , acc: 98.36 %\n",
      "[epoch: 9, batch_idx: 25]: loss: 0.000 , acc: 98.36 %\n",
      "[epoch: 9, batch_idx: 30]: loss: 0.000 , acc: 98.67 %\n",
      "[epoch: 9, batch_idx: 35]: loss: 0.000 , acc: 99.06 %\n",
      "[epoch: 9, batch_idx: 40]: loss: 0.000 , acc: 99.06 %\n",
      "[epoch: 9, batch_idx: 45]: loss: 0.000 , acc: 98.67 %\n",
      "[epoch: 9, batch_idx: 50]: loss: 0.000 , acc: 99.22 %\n",
      "[epoch: 9, batch_idx: 55]: loss: 0.000 , acc: 98.75 %\n",
      "[epoch: 9, batch_idx: 60]: loss: 0.000 , acc: 98.36 %\n",
      "[epoch: 9, batch_idx: 65]: loss: 0.000 , acc: 98.59 %\n",
      "[epoch: 9, batch_idx: 70]: loss: 0.001 , acc: 98.59 %\n",
      "[epoch: 9, batch_idx: 75]: loss: 0.000 , acc: 99.14 %\n",
      "[epoch: 9, batch_idx: 80]: loss: 0.000 , acc: 98.75 %\n",
      "[epoch: 9, batch_idx: 85]: loss: 0.000 , acc: 99.22 %\n",
      "[epoch: 9, batch_idx: 90]: loss: 0.000 , acc: 98.98 %\n",
      "[epoch: 9, batch_idx: 95]: loss: 0.000 , acc: 97.89 %\n",
      "[epoch: 9, batch_idx: 100]: loss: 0.001 , acc: 98.52 %\n",
      "[epoch: 9, batch_idx: 105]: loss: 0.001 , acc: 98.44 %\n",
      "[epoch: 9, batch_idx: 110]: loss: 0.001 , acc: 98.52 %\n",
      "[epoch: 9, batch_idx: 115]: loss: 0.000 , acc: 98.98 %\n",
      "[epoch: 9, batch_idx: 120]: loss: 0.000 , acc: 98.75 %\n",
      "[epoch: 9, batch_idx: 125]: loss: 0.000 , acc: 98.98 %\n",
      "[epoch: 9, batch_idx: 130]: loss: 0.001 , acc: 98.91 %\n",
      "[epoch: 9, batch_idx: 135]: loss: 0.000 , acc: 98.52 %\n",
      "[epoch: 9, batch_idx: 140]: loss: 0.000 , acc: 98.91 %\n",
      "[epoch: 9, batch_idx: 145]: loss: 0.000 , acc: 98.83 %\n",
      "[epoch: 9, batch_idx: 150]: loss: 0.000 , acc: 99.22 %\n",
      "[epoch: 9, batch_idx: 155]: loss: 0.000 , acc: 99.06 %\n",
      "[epoch: 9, batch_idx: 160]: loss: 0.000 , acc: 98.91 %\n",
      "[epoch: 9, batch_idx: 165]: loss: 0.000 , acc: 98.83 %\n",
      "[epoch: 9, batch_idx: 170]: loss: 0.000 , acc: 99.14 %\n",
      "[epoch: 9, batch_idx: 175]: loss: 0.000 , acc: 98.98 %\n",
      "[epoch: 9, batch_idx: 180]: loss: 0.000 , acc: 99.61 %\n",
      "[epoch: 9, batch_idx: 185]: loss: 0.000 , acc: 99.06 %\n",
      "[epoch: 9, batch_idx: 190]: loss: 0.000 , acc: 98.59 %\n",
      "[epoch: 9, batch_idx: 195]: loss: 0.001 , acc: 98.44 %\n",
      "[epoch: 9, batch_idx: 200]: loss: 0.000 , acc: 98.67 %\n",
      "[epoch: 9, batch_idx: 205]: loss: 0.000 , acc: 98.59 %\n",
      "[epoch: 9, batch_idx: 210]: loss: 0.000 , acc: 99.30 %\n",
      "[epoch: 9, batch_idx: 215]: loss: 0.000 , acc: 98.75 %\n",
      "[epoch: 9, batch_idx: 220]: loss: 0.000 , acc: 99.30 %\n",
      "[epoch: 9, batch_idx: 225]: loss: 0.000 , acc: 98.98 %\n",
      "[epoch: 9, batch_idx: 230]: loss: 0.000 , acc: 98.20 %\n",
      "[epoch: 9, batch_idx: 235]: loss: 0.000 , acc: 98.84 %\n",
      "[batch_index: 0]: Accuracy on val set: 98.1 % \n",
      "[batch_index: 1]: Accuracy on val set: 97.8 % \n",
      "[batch_index: 2]: Accuracy on val set: 98.5 % \n",
      "[batch_index: 3]: Accuracy on val set: 98.1 % \n",
      "[batch_index: 4]: Accuracy on val set: 98.4 % \n",
      "[batch_index: 5]: Accuracy on val set: 99.6 % \n",
      "[batch_index: 6]: Accuracy on val set: 99.0 % \n",
      "[batch_index: 7]: Accuracy on val set: 99.9 % \n",
      "[batch_index: 8]: Accuracy on val set: 99.8 % \n",
      "[batch_index: 9]: Accuracy on val set: 99.1 % \n",
      "Average accuracy on val set: 98.8 % \n",
      "[epoch: 10, batch_idx: 5]: loss: 0.001 , acc: 98.87 %\n",
      "[epoch: 10, batch_idx: 10]: loss: 0.000 , acc: 98.67 %\n",
      "[epoch: 10, batch_idx: 15]: loss: 0.000 , acc: 98.67 %\n",
      "[epoch: 10, batch_idx: 20]: loss: 0.000 , acc: 98.98 %\n",
      "[epoch: 10, batch_idx: 25]: loss: 0.000 , acc: 98.59 %\n",
      "[epoch: 10, batch_idx: 30]: loss: 0.001 , acc: 98.20 %\n",
      "[epoch: 10, batch_idx: 35]: loss: 0.000 , acc: 98.83 %\n",
      "[epoch: 10, batch_idx: 40]: loss: 0.001 , acc: 98.75 %\n",
      "[epoch: 10, batch_idx: 45]: loss: 0.000 , acc: 99.22 %\n",
      "[epoch: 10, batch_idx: 50]: loss: 0.001 , acc: 98.75 %\n",
      "[epoch: 10, batch_idx: 55]: loss: 0.000 , acc: 98.83 %\n",
      "[epoch: 10, batch_idx: 60]: loss: 0.000 , acc: 98.75 %\n",
      "[epoch: 10, batch_idx: 65]: loss: 0.000 , acc: 98.91 %\n",
      "[epoch: 10, batch_idx: 70]: loss: 0.000 , acc: 99.22 %\n",
      "[epoch: 10, batch_idx: 75]: loss: 0.000 , acc: 98.91 %\n",
      "[epoch: 10, batch_idx: 80]: loss: 0.001 , acc: 98.98 %\n",
      "[epoch: 10, batch_idx: 85]: loss: 0.000 , acc: 98.98 %\n",
      "[epoch: 10, batch_idx: 90]: loss: 0.001 , acc: 98.44 %\n",
      "[epoch: 10, batch_idx: 95]: loss: 0.000 , acc: 99.14 %\n",
      "[epoch: 10, batch_idx: 100]: loss: 0.000 , acc: 98.67 %\n",
      "[epoch: 10, batch_idx: 105]: loss: 0.000 , acc: 99.06 %\n",
      "[epoch: 10, batch_idx: 110]: loss: 0.001 , acc: 98.75 %\n",
      "[epoch: 10, batch_idx: 115]: loss: 0.000 , acc: 99.06 %\n",
      "[epoch: 10, batch_idx: 120]: loss: 0.000 , acc: 99.38 %\n",
      "[epoch: 10, batch_idx: 125]: loss: 0.001 , acc: 98.67 %\n",
      "[epoch: 10, batch_idx: 130]: loss: 0.000 , acc: 99.45 %\n",
      "[epoch: 10, batch_idx: 135]: loss: 0.000 , acc: 98.75 %\n",
      "[epoch: 10, batch_idx: 140]: loss: 0.000 , acc: 99.14 %\n",
      "[epoch: 10, batch_idx: 145]: loss: 0.000 , acc: 99.06 %\n",
      "[epoch: 10, batch_idx: 150]: loss: 0.000 , acc: 98.91 %\n",
      "[epoch: 10, batch_idx: 155]: loss: 0.001 , acc: 98.67 %\n",
      "[epoch: 10, batch_idx: 160]: loss: 0.000 , acc: 98.28 %\n",
      "[epoch: 10, batch_idx: 165]: loss: 0.000 , acc: 99.45 %\n",
      "[epoch: 10, batch_idx: 170]: loss: 0.001 , acc: 98.75 %\n",
      "[epoch: 10, batch_idx: 175]: loss: 0.000 , acc: 98.75 %\n",
      "[epoch: 10, batch_idx: 180]: loss: 0.000 , acc: 99.22 %\n",
      "[epoch: 10, batch_idx: 185]: loss: 0.000 , acc: 98.75 %\n",
      "[epoch: 10, batch_idx: 190]: loss: 0.000 , acc: 99.38 %\n",
      "[epoch: 10, batch_idx: 195]: loss: 0.000 , acc: 99.14 %\n",
      "[epoch: 10, batch_idx: 200]: loss: 0.000 , acc: 99.14 %\n",
      "[epoch: 10, batch_idx: 205]: loss: 0.000 , acc: 99.30 %\n",
      "[epoch: 10, batch_idx: 210]: loss: 0.000 , acc: 98.98 %\n",
      "[epoch: 10, batch_idx: 215]: loss: 0.000 , acc: 99.14 %\n",
      "[epoch: 10, batch_idx: 220]: loss: 0.000 , acc: 99.22 %\n",
      "[epoch: 10, batch_idx: 225]: loss: 0.000 , acc: 98.83 %\n",
      "[epoch: 10, batch_idx: 230]: loss: 0.001 , acc: 98.75 %\n",
      "[epoch: 10, batch_idx: 235]: loss: 0.000 , acc: 98.57 %\n",
      "[batch_index: 0]: Accuracy on val set: 98.8 % \n",
      "[batch_index: 1]: Accuracy on val set: 97.6 % \n",
      "[batch_index: 2]: Accuracy on val set: 98.3 % \n",
      "[batch_index: 3]: Accuracy on val set: 98.6 % \n",
      "[batch_index: 4]: Accuracy on val set: 98.6 % \n",
      "[batch_index: 5]: Accuracy on val set: 99.4 % \n",
      "[batch_index: 6]: Accuracy on val set: 99.1 % \n",
      "[batch_index: 7]: Accuracy on val set: 99.6 % \n",
      "[batch_index: 8]: Accuracy on val set: 99.6 % \n",
      "[batch_index: 9]: Accuracy on val set: 99.0 % \n",
      "Average accuracy on val set: 98.9 % \n"
     ]
    }
   ],
   "source": [
    "train_loss = 0.0  # 这整个epoch的loss清零\n",
    "total = 0\n",
    "correct = 0\n",
    "epoch = 10\n",
    "#log to record loss and acc\n",
    "log=np.zeros([epoch,3])#train_loss,val_loss,val_accuracy\n",
    "iter_num = 0\n",
    "for i in range(epoch):\n",
    "    # print(i)\n",
    "    train_loss_list = []\n",
    "    ## real dataset的训练方法\n",
    "    for batch_idx, (data, labels) in enumerate(data_real_train_loader):\n",
    "        data = data.to(device)\n",
    "        labels = labels.to(device)\n",
    "    ## 本项目的训练方法\n",
    "    # for batch_idx, traindata in enumerate(data_train_loader):\n",
    "        iter_num += 1\n",
    "        optimizer.zero_grad()\n",
    "        # forward + backward + update\n",
    "        ## 本项目的训练方法\n",
    "        # data, labels = traindata['data'].to(device), traindata['label'].to(device)\n",
    "        # print(data.shape)\n",
    "        outputs = model.forward(data)\n",
    "        # print(outputs.shape, labels.shape)\n",
    "        loss = nf.cross_entropy(outputs, labels)\n",
    "        writer.add_scalar(\"Loss/train\", loss, iter_num)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # 把运行中的loss累加起来\n",
    "        train_loss_list += [loss.item()]\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, dim=1)\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += data.shape[0]\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "        if batch_idx % 5 == 4:  # 不想要每一次都出loss，浪费时间，选择每5次出一个平均损失,和准确率\n",
    "            print('[epoch: %d, batch_idx: %d]: loss: %.3f , acc: %.2f %%'\n",
    "                    % (i + 1, batch_idx + 1, loss / 100, 100. * correct / total))\n",
    "            writer.add_scalar('train accuracy per 10 batches', 100. * correct / total, iter_num)\n",
    "            loss = 0.0  \n",
    "            correct = 0  \n",
    "            total = 0\n",
    "    scheduler.step()  # 优化并更新学习率\n",
    "    log[i,0]=np.mean(train_loss_list)\n",
    "    ## 模型验证\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval() # 切换模型为测试状态(没加drop_out层，因此这句话可以随便注释掉)\n",
    "    val_loss = []\n",
    "    with torch.no_grad():  # 测试集不用算梯度\n",
    "        ## real dataset的训练方法\n",
    "        for batch_idx, (data, labels) in enumerate(data_real_test_loader):\n",
    "            data = data.to(device)\n",
    "            labels = labels.to(device)\n",
    "        ## 本项目的训练方法\n",
    "        # for batch_idx, valdata in enumerate(data_test_loader ):\n",
    "            ## 本项目的训练方法\n",
    "            # data, labels = valdata['data'].to(device), valdata['label'].to(device)\n",
    "            outputs = model(data)\n",
    "            _, predicted = torch.max(outputs.data, dim=1)  # dim = 1 列是第0个维度，行是第1个维度，沿着行(第1个维度)去找1.最大值和2.最大值的下标\n",
    "            loss = nf.cross_entropy(outputs, labels)\n",
    "            val_loss.append(loss.item())\n",
    "            total += labels.size(0)  # 张量之间的比较运算\n",
    "            correct_batch = predicted.eq(labels).sum().item()\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "            acc_batch = correct_batch / labels.size(0)\n",
    "            # print(labels.size(0))\n",
    "            print('[batch_index: %d]: Accuracy on val set: %.1f %% ' % (batch_idx, 100 * acc_batch))  # 求测试的准确率，正确数/总数\n",
    "            predicted_list = predicted.tolist()\n",
    "            targets_list = labels.tolist()\n",
    "            writer.add_scalar('val accuracy per batch', 100 * acc_batch, batch_idx)\n",
    "    acc = correct / total\n",
    "    print('Average accuracy on val set: %.1f %% ' % (100. * acc))  # 求测试的准确率，正确数/总数\n",
    "    log[i,1]= np.mean(val_loss)\n",
    "    log[i,2]= acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part6.绘制训练与验证时的准确率函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.58924128 0.11413793 0.9657    ]\n",
      " [0.10532643 0.07480799 0.9779    ]\n",
      " [0.07644349 0.05689508 0.9824    ]\n",
      " [0.06291693 0.05335503 0.9842    ]\n",
      " [0.05413151 0.04587625 0.9857    ]\n",
      " [0.05072334 0.04204028 0.9876    ]\n",
      " [0.04388573 0.0447113  0.9858    ]\n",
      " [0.04077688 0.03644895 0.9883    ]\n",
      " [0.0383473  0.0366402  0.9884    ]\n",
      " [0.03461222 0.03499802 0.9887    ]]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAcoAAAE5CAYAAAAQmmBrAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8g+/7EAAAACXBIWXMAAA9hAAAPYQGoP6dpAABOxklEQVR4nO3deVxU5f4H8M8ZlmFHEAVUcEVFBFzABMsVRFESu6mhpV7Tfrer5a5ZaS43MXfNXCvNrqhpYqa4IIkmmuKCuRBpgaAXxBVkH2bO7w9kdGQYZkZgAD/v1+u8mPOc5znnex4GvnPOnHMeQRRFEURERKSWxNABEBER1WRMlERERBowURIREWnARElERKQBEyUREZEGTJREREQaMFESERFpwERJRESkARMlERGRBkyUVOc1a9YMK1eu1Lp+bGwsBEHAo0ePqiwmANiyZQvq1atXpdswBEEQsHfvXkOHQVRpmCipxhAEQeM0d+5cvdYbHx+P9957T+v6/v7+SE9Ph62trV7bI6K6xdjQARCVSk9PV77euXMn5syZg6SkJGWZlZWV8rUoipDL5TA2rvgt3KBBA53iMDU1hZOTk05tqHaTyWQwMTExdBhUQ/GIkmoMJycn5WRrawtBEJTzf/zxB6ytrXHw4EF07twZUqkUJ0+exF9//YVBgwbB0dERVlZW8PX1xdGjR1XW+/ypV0EQ8PXXX2Pw4MGwsLCAm5sb9u3bp1z+/KnX0lOkhw8fhru7O6ysrNCvXz+VxF5cXIwPP/wQ9erVQ/369TFz5kyMGjUKoaGhOvXBunXr0LJlS5iamqJNmzb4/vvvlctEUcTcuXPh6uoKqVSKRo0a4cMPP1QuX7t2Ldzc3GBmZgZHR0e8+eab5W7n/v37CAsLQ+PGjWFhYQFPT09s375dpU7Pnj3x4YcfYsaMGbC3t4eTk1OZo/rr16+je/fuMDMzQ7t27RAdHV3hPh46dAivvvqqsq8GDhyIv/76S6XOrVu3EBYWBnt7e1haWsLHxwdnzpxRLv/555/h6+sLMzMzODg4YPDgwcpl6k791qtXD1u2bAEApKSkQBAE7Ny5Ez169ICZmRm2bdumVZ8oFAosXrwYrVq1glQqhaurKz7//HMAQO/evTFhwgSV+nfv3oWpqSliYmIq7BequZgoqVb56KOPsGjRIiQmJsLLyws5OTkIDg5GTEwMLl68iH79+iEkJASpqaka1zNv3jwMHToUv//+O4KDgzFixAg8ePCg3Pp5eXlYunQpvv/+e5w4cQKpqamYNm2acvkXX3yBbdu2YfPmzYiLi0N2drbO39NFRkZi4sSJmDp1Kq5cuYL/+7//wz//+U8cO3YMAPDjjz9ixYoV2LBhA65fv469e/fC09MTAHDu3Dl8+OGHmD9/PpKSknDo0CF079693G0VFBSgc+fOOHDgAK5cuYL33nsP77zzDs6ePatS77vvvoOlpSXOnDmDxYsXY/78+cpkqFAo8MYbb8DU1BRnzpzB+vXrMXPmzAr3Mzc3F1OmTMG5c+cQExMDiUSCwYMHQ6FQAABycnLQo0cP3L59G/v27cOlS5cwY8YM5fIDBw5g8ODBCA4OxsWLFxETE4MuXbro1NdAyXtp4sSJSExMRFBQkFZ9MmvWLCxatAizZ8/GtWvXEBERAUdHRwDA2LFjERERgcLCQmX9//73v2jcuDF69+6tc3xUg4hENdDmzZtFW1tb5fyxY8dEAOLevXsrbOvh4SF++eWXyvmmTZuKK1asUM4DED/99FPlfE5OjghAPHjwoMq2Hj58qIwFgHjjxg1lm6+++kp0dHRUzjs6OopLlixRzhcXF4uurq7ioEGDtN5Hf39/cdy4cSp1hgwZIgYHB4uiKIrLli0TW7duLRYVFZVZ148//ija2NiI2dnZ5W6vIgMGDBCnTp2qnO/Ro4f46quvqtTx9fUVZ86cKYqiKB4+fFg0NjYWb9++rVx+8OBBEYAYGRmp9Xbv3r0rAhAvX74siqIobtiwQbS2thbv37+vtr6fn584YsSIctenbvu2trbi5s2bRVEUxeTkZBGAuHLlygpje7ZPsrOzRalUKm7atElt3fz8fNHOzk7cuXOnsszLy0ucO3duhduhmo1HlFSr+Pj4qMzn5ORg2rRpcHd3R7169WBlZYXExMQKjyi9vLyUry0tLWFjY4PMzMxy61tYWKBly5bKeWdnZ2X9rKws3LlzR+WoxsjICJ07d9Zp3xITE9GtWzeVsm7duiExMREAMGTIEOTn56NFixYYN24cIiMjUVxcDAAIDAxE06ZN0aJFC7zzzjvYtm0b8vLyyt2WXC7HggUL4OnpCXt7e1hZWeHw4cNl+u3Zfnp+vxMTE+Hi4oJGjRopl/v5+VW4n9evX0dYWBhatGgBGxsbNGvWDACU205ISEDHjh1hb2+vtn1CQgL69OlT4XYq8vx7qaI+SUxMRGFhYbnbNjMzwzvvvINvv/0WAHDhwgVcuXIFo0ePfuFYybCYKKlWsbS0VJmfNm0aIiMjsXDhQvz6669ISEiAp6cnioqKNK7n+Qs3BEFQntrTtr5YzWOeu7i4ICkpCWvXroW5uTn+/e9/o3v37pDJZLC2tsaFCxewfft2ODs7Y86cOfD29i73FpclS5Zg1apVmDlzJo4dO4aEhAQEBQWV6Tdd+0kbISEhePDgATZt2oQzZ84ov3ss3ba5ubnG9hUtV/e7kclkZeo9/16qqE8q2i5Qcvo1Ojoat27dwubNm9G7d280bdq0wnZUszFRUq0WFxeH0aNHY/DgwfD09ISTkxNSUlKqNQZbW1s4OjoiPj5eWSaXy3HhwgWd1uPu7o64uDiVsri4OLRr1045b25ujpCQEKxevRqxsbE4ffo0Ll++DAAwNjZGQEAAFi9ejN9//x0pKSn45Zdf1G4rLi4OgwYNwttvvw1vb2+0aNECf/75p87xpqWlqVzU9Ntvv2lsc//+fSQlJeHTTz9Fnz594O7ujocPH6rU8fLyQkJCQrnfGXt5eWm8OKZBgwYqMV2/fl3j0XWpivrEzc0N5ubmGrft6ekJHx8fbNq0CRERERgzZkyF26Waj7eHUK3m5uaGPXv2ICQkBIIgYPbs2S98xKOPDz74AOHh4WjVqhXatm2LL7/8Eg8fPoQgCFqvY/r06Rg6dCg6duyIgIAA/Pzzz9izZ4/yKt4tW7ZALpfjlVdegYWFBf773//C3NwcTZs2xf79+/H333+je/fusLOzQ1RUFBQKBdq0aaN2W25ubti9ezdOnToFOzs7LF++HHfu3FFJyhUJCAhA69atMWrUKCxZsgTZ2dn45JNPNLaxs7ND/fr1sXHjRjg7OyM1NRUfffSRSp2wsDAsXLgQoaGhCA8Ph7OzMy5evIhGjRrBz88Pn332Gfr06YOWLVvirbfeQnFxMaKiopQXEvXu3Rtr1qyBn58f5HI5Zs6cqdWtHxX1iZmZGWbOnIkZM2bA1NQU3bp1w927d3H16lW8++67yvWMHTsWEyZMgKWlpcrVuFR78YiSarXly5fDzs4O/v7+CAkJQVBQEDp16lTtccycORNhYWEYOXIk/Pz8YGVlhaCgIJiZmWm9jtDQUKxatQpLly6Fh4cHNmzYgM2bN6Nnz54ASm5x2LRpE7p16wYvLy8cPXoUP//8M+rXr4969ephz5496N27N9zd3bF+/Xps374dHh4earf16aefolOnTggKCkLPnj3h5OSk860sEokEkZGRyM/PR5cuXTB27FjlrRKa2uzYsQPnz59H+/btMXnyZCxZskSljqmpKY4cOYKGDRsiODgYnp6eWLRoEYyMjACU3Laya9cu7Nu3Dx06dEDv3r1VrkxdtmwZXFxc8Nprr2H48OGYNm0aLCwsKtwfbfpk9uzZmDp1KubMmQN3d3cMGzaszHfbYWFhMDY2RlhYmE6/f6q5BLG6v2ghegkoFAq4u7tj6NChWLBggaHDoWqUkpKCli1bIj4+3iAf2qjy8dQrUSW4efMmjhw5gh49eqCwsBBr1qxBcnIyhg8fbujQqJrIZDLcv38fn376Kbp27cokWYfw1CtRJZBIJNiyZQt8fX3RrVs3XL58GUePHoW7u7uhQ6NqEhcXB2dnZ8THx2P9+vWGDocqEU+9EhERaVBjjigXLVoEQRAwadIkjfV27dqFtm3bwszMDJ6enoiKiqqeAImI6KVUIxJlfHw8NmzYUOYpIM87deoUwsLC8O677+LixYsIDQ1FaGgorly5Uk2REhHRy8bgp15zcnLQqVMnrF27Fv/5z3/QoUOHcgfZHTZsGHJzc7F//35lWdeuXdGhQwd+J0BERFXC4Fe9jh8/HgMGDEBAQAD+85//aKx7+vRpTJkyRaUsKChI4ygNhYWFKk/zVygUePDgAerXr6/TzeBERFS3iKKIx48fo1GjRpBIyj/BatBEuWPHDly4cEHl0V+aZGRkKIe0KeXo6IiMjIxy24SHh2PevHkvFCcREdVdaWlpaNKkSbnLDZYo09LSMHHiRERHR1fp0ytmzZqlchSalZUFV1dXJCcnw9rausq2a0gymQzHjh1Dr169OGq7Dthv+mPf6Yf9pp/K6rfHjx+jefPmFeYCgyXK8+fPIzMzU+WmXLlcjhMnTmDNmjUoLCxUPrKqlJOTE+7cuaNSdufOHTg5OZW7HalUCqlUWqbc3t4eNjY2L7gXNZNMJoOFhQXq16/PPz4dsN/0x77TD/tNP5XVb6VtK/oazmBXvfbp0weXL19GQkKCcvLx8cGIESOQkJBQJkkCJWPdPf/k/ujoaK3GwCMiItKHwY4ora2t0b59e5UyS0tL1K9fX1k+cuRING7cGOHh4QCAiRMnokePHli2bBkGDBiAHTt24Ny5c9i4cWO1x09ERC+HGnEfZXlSU1NVxpXz9/dHREQENm7cCG9vb+zevRt79+4tk3CJiIgqi8FvD3lWbGysxnkAGDJkCIYMGVI9ARHRS08URRQXF0Mul1f6umUyGYyNjVFQUFAl66+rtO03IyMjGBsbv/CtgDUqURIR1SRFRUVIT09HXl5elaxfFEU4OTkhLS2N93XrQJd+s7CwgLOzM0xNTfXeHhMlEZEaCoUCycnJMDIyQqNGjWBqalrpyUyhUCAnJwdWVlYab3gnVdr0myiKKCoqwt27d5GcnAw3Nze9+5iJkohIjaKiIigUCri4uMDCwqJKtqFQKFBUVAQzMzMmSh1o22/m5uYwMTHBzZs3lfX1wd8MEZEGTGC1W2X8/vgOICIi0oCJkoiISAMmSiIi0qhZs2blDn9YneswFF7MQ0RUx/Ts2VPj2L66io+Ph6WlZaWsqzZioiQiegmJogi5XA5j44rTQIMGDaohopqLp16JiOqQ0aNH4/jx41i1ahUEQYAgCEhJSUFsbCwEQcDBgwfRuXNnSKVSnDx5En/99RcGDRoER0dHWFlZwdfXF0ePHlVZ5/OnTQVBwNdff43BgwfDwsICbm5u2Ldvn05xpqamYtCgQbCysoKNjQ2GDh2qMjrUpUuX0KtXL1hbW8PGxgadO3fGuXPnAAA3b97EW2+9hfr168PS0hIeHh6IiorSv9MqwCNKIiJd+PgAGgaL14UAwEYUtXuQgZMT8CRRaLJq1Sr8+eefaN++PebPnw+g5IgwJSUFAPDRRx9h6dKlaNGiBezs7JCWlobg4GB8/vnnkEql2Lp1K0JCQpCUlARXV9dytzNv3jwsXrwYS5YswZdffokRI0bg5s2bsLe3rzBGhUKhTJLHjx9HcXExxo8fj2HDhikfXTpixAh07NgR69atg5GRERISEpTDYk2YMAFFRUWIjY2FtbU1rl27Bisrqwq3qy8mSiIiXWRkALdvV8qqhCdTZbK1tYWpqSksLCzUjtU7f/58BAYGKuft7e3h7e2tnF+wYAEiIyOxb98+TJgwodztjB49GmFhYQCAhQsXYvXq1Th79iz69etXYYwxMTG4fPkykpOT4eLiAgDYunUrPDw8EB8fD19fX6SmpmL69Olo27YtAMDNzU3ZPi0tDQMGDICnpyckEglatGhR4TZfBBMlEZEuNAwUrysRJd8VCoJQccKspO36+PiozOfk5GDu3Lk4cOAA0tPTUVxcjPz8fKSmpmpcj5eXl/K1paUlbGxskJmZqVUMiYmJcHFxUSZJAGjXrh3q1auHxMRE+Pr6YsqUKRg7diy+//57BAQEYMiQIWjZsiWAkiPK8ePH48SJEwgICMA//vEPlXgqGxMlEZEutDj9qS1RoUB2djZsbGwgVNMTgJ6/enXatGmIjo7G0qVL0apVK5ibm+PNN99EUVGRxvWUngYtJQgCFApFpcU5d+5cDB8+HAcOHMDBgwfx2WefYceOHRg8eDDGjh0Lf39/nDhxAkePHkV4eDiWLVuGDz74oNK2/yxezENEVMeYmppqPWxXXFwcRo8ejcGDB8PT0xNOTk7K7zOriru7O9LS0pCWlqYsu3btGh49eoR27dopy1q3bo3JkyfjyJEjeOONN7B582blsiZNmuBf//oX9uzZg6lTp2LTpk1VFi8TJRFRHdOsWTOcOXMGKSkpuHfvnsYjPTc3N+zZswcJCQm4dOkShg8fXqlHhuoEBATA09MTI0aMwIULF3D27FmMHDkSPXr0gI+PD/Lz8zFhwgTExsbi5s2biIuLQ3x8PNzd3QEAkydPRkxMDJKTk3HhwgUcO3ZMuawqMFESEdUx06ZNg5GREdq1a4cGDRpo/L5x+fLlsLOzg7+/P0JCQhAUFIROnTpVaXyCIOCnn36CnZ0dunfvjoCAALRo0QI7d+4EUDLg8v379zFy5Ei0bt0aQ4cORf/+/TFv3jwAgFwux/Tp0+Hh4YF+/fqhdevWWLt2bdXFK4qiWGVrr4Gys7Nha2uLrKws2NjYGDqcKiGTyRAVFYXg4OAy3yNQ+dhv+quLfVdQUIDk5GQ0b95c7+GZKqJ45jtKjlKiPV36TdPvUdt8YNDfzLp16+Dl5QUbGxvY2NjAz88PBw8eLLf+li1blDfQlk5V9QYmIiICDHzVa5MmTbBo0SK4ublBFEV89913GDRoEC5evAgPDw+1bWxsbJCUlKScr+wRx4mIiJ5l0EQZEhKiMv/5559j3bp1+O2338pNlIIgqL2JloiIqCrUmPso5XI5du3ahdzcXPj5+ZVbLycnB02bNoVCoUCnTp2wcOHCcpMqABQWFqKwsFA5n52dDaDkOxWZTFZ5O1CDlO5XXd2/qsJ+019d7DuZTAZRFKFQKKrsKtDSS0RKt0Pa0aXfFAoFRFGETCaDkZGRyjJt368Gv5jn8uXL8PPzQ0FBAaysrBAREYHg4GC1dU+fPo3r16/Dy8sLWVlZWLp0KU6cOIGrV6+iSZMmatvMnTtXeaXUsyIiImBhYVGp+0JEdYexsTGcnJzg4uICU1NTQ4dDeioqKkJaWhoyMjJQXFyssiwvLw/Dhw+v8GIegyfKoqIipKamIisrC7t378bXX3+N48ePq9x0Wh6ZTAZ3d3eEhYVhwYIFauuoO6J0cXHBvXv36vRVr9HR0QgMDKwzVyBWB/ab/upi3xUUFCAtLQ3NmjWrsosGRVHE48ePYW1tzestdKBLvxUUFCAlJQUuLi5qr3p1cHCoMFEa/NSrqakpWrVqBQDo3Lkz4uPjsWrVKmzYsKHCtiYmJujYsSNu3LhRbh2pVAqpVKq2bV35gy7Py7CPVYH9pr+61HdyuRyCIEAikVTZrRulpw1Lt0Pa0aXfJBIJBEFQ+97U9r1a434zCoVC5QhQE7lcjsuXL8PZ2bmKoyIiopeVQY8oZ82ahf79+8PV1RWPHz9GREQEYmNjcfjwYQDAyJEj0bhxY4SHhwMoGR6ma9euaNWqFR49eoQlS5bg5s2bGDt2rCF3g4iI6jCDJsrMzEyMHDkS6enpsLW1hZeXFw4fPqwcKy01NVXlsPrhw4cYN24cMjIyYGdnh86dO+PUqVNafZ9JRETaa9asGSZNmoRJkyapXT569Gg8evQIe/furda4DMGgifKbb77RuLx0pOtSK1aswIoVK6owIiIiIlU17jtKIiKimoSJkoioDtm4cSMaNWpU5kb8QYMGYcyYMQCAv/76C4MGDYKjoyOsrKzg6+uLo0ePvtB2CwsL8eGHH6Jhw4YwMzPDq6++ivj4eOXyhw8fYsSIEWjQoAHMzc3h5uamHF+yqKgIEyZMgLOzM8zMzNC0aVPltSk1gcFvDyEiqk18fICMjMpamwBRtNHqHkonJ+DcuYrXOGTIEHzwwQc4duwY+vTpAwB48OABDh06hKioKAAlTzgLDg7G559/DqlUiq1btyIkJARJSUlwdXXVa09mzJiBH3/8Ed999x2aNm2KxYsXIygoCDdu3IC9vT1mz56Na9eu4eDBg3BwcMCNGzeQn58PAFi9ejX27duHH374Aa6urmUGdTY0JkoiIh1kZAC3b1fW2oQnU+Wxs7ND//79ERERoUyUu3fvhoODA3r16gUA8Pb2hre3t7LNggULEBkZiX379mHChAk6bzM3Nxfr1q3Dli1b0L9/fwDApk2bEB0djW+++QbTp09HamoqOnbsCB8fHwAlFwuVSk1NhZubG1599VUIgoCmTZvqu/tVgomSiEgHlTsmgwhRFJ8cUWpOmLpsd8SIERg3bhzWrl0LqVSKbdu24a233lLeRZCTk4O5c+fiwIEDSE9PR3FxMfLz8zUO8KzJX3/9BZlMhm7duinLTExM0KVLFyQmJgIA3n//ffzjH//AhQsX0LdvX4SGhsLf3x9AyRW0gYGBaNOmDfr164eBAweib9++esVSFZgoiYh0oM3pT20pFOIzAxBX3pFlSEgIRFHEgQMH4Ovri19//VXljoFp06YhOjoaS5cuRatWrWBubo4333wTRUVFlRbD8/r374+bN28iKioK0dHR6NOnD8aPH4+lS5eiU6dOSE5OxsGDB3H06FEMHToUAQEB2L17d5XFowtezENEVMeYmZnhjTfewLZt27B9+3a0adMGnTp1Ui6Pi4vD6NGjMXjwYHh6esLJyQkpKSl6b69ly5YwNTVFXFycskwmkyE+Pl7lPvcGDRpg1KhR+O9//4uVK1di48aNymU2NjYYNmwYNm3ahJ07d+LHH3/EgwcP9I6pMvGIkoioDhoxYgQGDhyIq1ev4u2331ZZ5ubmhj179iAkJASCIGD27NkvNMyXpaUl3n//fUyfPh329vZwdXXF4sWLkZeXh3fffRcAMGfOHHTu3BkeHh4oLCzE/v374e7uDgBYvnw5nJ2d0bFjR0gkEuzatQtOTk6oV6+e3jFVJiZKIqI6qHfv3rC3t0dSUhKGDx+usmz58uUYM2YM/P394eDggJkzZyrH6tXXokWLoFAo8M477+Dx48fw8fHB4cOHYWdnB6BkAIxZs2YhJSUF5ubmeO2117Bjxw4AgLW1NRYvXozr16/DyMgIvr6+iIqKqjEPijf4MFvVLTs7G7a2thUOq1KbyWQyREVFITg4uM6M5FAd2G/6q4t9V1BQgOTkZDRv3rzKhtlSKBTPfEdZM5JCbaBLv2n6PWqbD/ibISIi0oCJkoiISAMmSiIiIg2YKImIiDRgoiQiItKAiZKIiEgDJkoiIiINmCiJiIg0MGiiXLduHby8vGBjYwMbGxv4+fnh4MGDGtvs2rULbdu2hZmZGTw9PZXjqxEREVUFgybKJk2aYNGiRTh//jzOnTuH3r17Y9CgQbh69ara+qdOnUJYWBjeffddXLx4EaGhoQgNDcWVK1eqOXIiorqtWbNmWLlypaHDqBEM+qzXkJAQlfnPP/8c69atw2+//QYPD48y9VetWoV+/fph+vTpAEoGG42OjsaaNWuwfv16tdsoLCxEYWGhcr70eYYymQwymayydqVGKd2vurp/VYX9pr+62HcymQyiKEKhULzQA8M1KX2CaOl2apqaHFfpz4riUygUEEURMpkMRkZGKsu0fb/WmIeiy+Vy7Nq1C7m5ufDz81Nb5/Tp05gyZYpKWVBQEPbu3VvuesPDwzFv3rwy5UeOHIGFhcULxVzTRUdHGzqEWon9pr+61HfGxsZwcnJCTk5OlY7TCACPHz+u0vXrQ6FQoKCg4IUfll6VtOm3oqIi5Ofn48SJEyguLlZZlpeXp9V2DJ4oL1++DD8/PxQUFMDKygqRkZEq45c9KyMjA46Ojipljo6OyMjIKHf9s2bNUkmu2dnZcHFxQd++fev0Q9Gjo6MRGBhYZx5QXR3Yb/qri31XUFCAtLQ0WFlZVdlD0UVRxOPHj2FtbQ1BqJyBmzdu3Ij58+cjNTVV5YHhoaGhqF+/Pr755hv89ddfmDp1Ks6cOYPc3Fy4u7vj888/R0BAgLK+RCKBmZlZuf8n4+Pj8cknnyAhIQEymQwdOnTAsmXLVMa9fPToET766CP89NNPyMrKQqtWrbBw4UIMHDgQQMm4mLNnz8bZs2chlUrh6+uL7du3K0ccKY8u/VZQUABzc3N0795d7UPRtWHwRNmmTRskJCQgKysLu3fvxqhRo3D8+PFyk6WupFIppFJpmXITE5M68wddnpdhH6sC+01/danv5HI5BEGARCIpM0LF8tPLsfz08grX0cm5E/aF7VMpe33767iQfkE5L4qi2n/2U/ymYIrflDLlFRk2bBgmTpyI48ePo0+fPgCABw8e4PDhw8qhq/Ly8jBgwAAsXLgQUqkUW7duxaBBg5CUlARXV1flukr3X53c3FyMHj0aPj4+EEURy5Ytw8CBA3H9+nVYW1tDoVBgwIABePz4Mf773/+iZcuWuHbtGoyMjCCRSJCQkIDAwECMGTMGq1atgrGxMY4dOwZRFCscEaT0dKum+EpJJBIIgqD2vante9XgidLU1BStWrUCAHTu3Bnx8fFYtWoVNmzYUKauk5MT7ty5o1J2584dODk5VUusREQAkF2YjduPb1dYz8XWpUzZ3by7WrXNLtTvlKednR369++PiIgIZaLcvXs3HBwc0KtXLwCAt7c3vL29lW0WLFiAyMhI7Nu3DxMmTNBqO71791aZ37hxI+rVq4fjx49j4MCBOHr0KM6ePYvExES0bt0aANCiRQtl/cWLF8PHxwdr165Vlqm7NqUmMHiifJ5CoVC5+OZZfn5+iImJwaRJk5Rl0dHR5X6nSURUFWykNmhs3bjCeg0sGqgte7ZteUeUNlL9vxoaMWIExo0bh7Vr10IqlWLbtm146623lEdfOTk5mDt3Lg4cOID09HQUFxcjPz8fqampWm/jzp07+PTTTxEbG4vMzEzI5XLk5eUp15GQkIAmTZook+TzEhISMGTIEL33sToZNFHOmjUL/fv3h6urKx4/foyIiAjExsbi8OHDAICRI0eicePGCA8PBwBMnDgRPXr0wLJlyzBgwADs2LED586dw8aNGw25G0T0ktH3tCgAlVOxVTVwc0hICERRxIEDB+Dr64tff/0VK1asUC6fNm0aoqOjsXTpUrRq1Qrm5uZ48803dbpoadSoUbh//z5WrVqFpk2bQiqVws/PT7kOc3Nzje0rWl6TGDRRZmZmYuTIkUhPT4etrS28vLxw+PBhBAYGAkCZL6P9/f0RERGBTz/9FB9//DHc3Nywd+9etG/f3lC7QERU45iZmeGNN97Atm3bcOPGDbRp00blIpu4uDiMHj0agwcPBlByhJmSkqLTNuLi4rB27VoEBwcDANLS0nDv3j3lci8vL9y6dQt//vmn2qNKLy8vxMTEqL0roaYxaKL85ptvNC6PjY0tUzZkyJBac7hORGQoI0aMwMCBA3H16lW8/fbbKsvc3NywZ88ehISEQBAEzJ49W+f7Jd3c3PD999/Dx8cH2dnZmD59uspRYo8ePdC9e3f84x//wPLly9GqVSv88ccfEAQB/fr1w6xZs+Dp6Yl///vf+Ne//gVTU1McO3YMQ4YMgYODQ6X0QWXhs16JiOqg3r17w97eHklJSRg+fLjKsuXLl8POzg7+/v4ICQlBUFCQyhGnNr755hs8fPgQnTp1wjvvvIMPP/wQDRs2VKnz448/wtfXF2FhYWjXrh1mzJgBuVwOAGjdujWOHDmCS5cuoUuXLvDz88NPP/0EY+Mad+lMzbuYh4iIXpxEIsH//vc/tcuaNWuGX375RaVs/PjxKvMVnYrt2LEj4uPjVcrefPNNlXl7e3t8++235a6jR48eiIuL07idmoBHlERERBowURIREWnARElERKQBEyUREZEGTJRERBqUDulEtVNl/P6YKImI1Ch9YLa2QzFRzVT6+3uRh/Xz9hAiIjWMjIxQr149ZGZmAgAsLCwqbSisUgqFAkVFRSgoKKjUR9jVddr0myiKyMvLQ2ZmJurVq1dm0GZdMFESEZWjdGSi0mRZ2URRRH5+PszNzSs9CddluvRbvXr1XniEKSZKIqJyCIIAZ2dnNGzYEDKZrNLXL5PJcOLECXTv3r3OjONZHbTtNxMTkxc6kizFRElEVAEjI6NK+Yerbr3FxcUwMzNjotRBdfcbT4oTERFpwERJRESkARMlERGRBkyUREREGjBREhERaWDQRBkeHg5fX19YW1ujYcOGCA0NRVJSksY2W7ZsgSAIKpOZmVk1RUxERC8bgybK48ePY/z48fjtt98QHR0NmUyGvn37Ijc3V2M7GxsbpKenK6ebN29WU8RERPSyMeh9lIcOHVKZ37JlCxo2bIjz58+je/fu5bYTBOGFn7RARESkjRr1wIGsrCwAgL29vcZ6OTk5aNq0KRQKBTp16oSFCxfCw8NDbd3CwkIUFhYq57OzswGUPNmhKp60UROU7ldd3b+qwn7TH/tOP+w3/VRWv2nbXhBryBgyCoUCr7/+Oh49eoSTJ0+WW+/06dO4fv06vLy8kJWVhaVLl+LEiRO4evUqmjRpUqb+3LlzMW/evDLlERERsLCwqNR9ICKi2iMvLw/Dhw9HVlYWbGxsyq1XYxLl+++/j4MHD+LkyZNqE155ZDIZ3N3dERYWhgULFpRZru6I0sXFBffu3dPYMbWZTCZDdHQ0AgMD+VgsHbDf9Me+0w/7TT+V1W/Z2dlwcHCoMFHWiFOvEyZMwP79+3HixAmdkiRQ8tDbjh074saNG2qXS6VSSKVSte3q+hvzZdjHqsB+0x/7Tj/sN/28aL9p29agV72KoogJEyYgMjISv/zyC5o3b67zOuRyOS5fvgxnZ+cqiJCIiF52Bj2iHD9+PCIiIvDTTz/B2toaGRkZAABbW1uYm5sDAEaOHInGjRsjPDwcADB//nx07doVrVq1wqNHj7BkyRLcvHkTY8eONdh+EBFR3WXQRLlu3ToAQM+ePVXKN2/ejNGjRwMAUlNTVUawfvjwIcaNG4eMjAzY2dmhc+fOOHXqFNq1a1ddYRMR0UvEoIlSm+uIYmNjVeZXrFiBFStWVFFEREREqvisVyIiIg2YKImIiDRgoiQiItKAiZKIiEgDJkoiIiINmCiJiIg00DtR3rhxA4cPH0Z+fj4A7W71ICIiqm10TpT3799HQEAAWrdujeDgYKSnpwMA3n33XUydOrXSAyQiIjIknRPl5MmTYWxsjNTUVJVhqoYNG1ZmIGYiIqLaTucn8xw5cgSHDx8uM8qHm5sbbt68WWmBERER1QQ6H1Hm5uaqHfD4wYMHaoezIiIiqs10TpSvvfYatm7dqpwXBAEKhQKLFy9Gr169KjU4IiIiQ9P51OvixYvRp08fnDt3DkVFRZgxYwauXr2KBw8eIC4uripiJCIiMhidjyjbt2+PP//8E6+++ioGDRqE3NxcvPHGG7h48SJatmxZFTESEREZjF7DbNna2uKTTz6p7FiIiIhqHJ0T5YkTJzQu7969u97BEBER1TQ6J8qePXuWKRMEQflaLpe/UEBEREQ1ic7fUT58+FBlyszMxKFDh+Dr64sjR45URYxEREQGo3OitLW1VZkcHBwQGBiIL774AjNmzNBpXeHh4fD19YW1tTUaNmyI0NBQJCUlVdhu165daNu2LczMzODp6YmoqChdd4OIiEgrlTZ6iKOjo1ZJ7lnHjx/H+PHj8dtvvyE6OhoymQx9+/ZFbm5uuW1OnTqFsLAwvPvuu7h48SJCQ0MRGhqKK1euvOguEBERlaHzd5S///67yrwoikhPT8eiRYvQoUMHndb1/LNht2zZgoYNG+L8+fPlXhS0atUq9OvXD9OnTwcALFiwANHR0VizZg3Wr1+v0/aJiIgqonOi7NChAwRBKDOsVteuXfHtt9++UDBZWVkAAHt7+3LrnD59GlOmTFEpCwoKwt69e9XWLywsRGFhoXI+OzsbACCTySCTyV4o3pqqdL/q6v5VFfab/th3+mG/6aey+k3b9jonyuTkZJV5iUSCBg0awMzMTNdVqVAoFJg0aRK6deuG9u3bl1svIyMDjo6OKmWOjo7IyMhQWz88PBzz5s0rU37kyBG1z6ytS6Kjow0dQq3EftMf+04/7Df9vGi/5eXlaVVP50TZtGlTnYPRxvjx43HlyhWcPHmyUtc7a9YslSPQ7OxsuLi4oG/fvrCxsanUbdUUMpkM0dHRCAwMhImJiaHDqTXYb/pj3+mH/aafyuq30jOMFdEqUa5evVrrDX/44Yda1y01YcIE7N+/HydOnCgzfNfznJyccOfOHZWyO3fuwMnJSW19qVSqdlQTExOTOv/GfBn2sSqw3/THvtMP+00/L9pv2rbVKlGuWLFCq5UJgqBTohRFER988AEiIyMRGxuL5s2bV9jGz88PMTExmDRpkrIsOjoafn5+Wm+XiIhIW1olyue/l6ws48ePR0REBH766SdYW1srv2e0tbWFubk5AGDkyJFo3LgxwsPDAQATJ05Ejx49sGzZMgwYMAA7duzAuXPnsHHjxiqJkYiIXm6Vdh+lPtatW4esrCz07NkTzs7Oymnnzp3KOqmpqUhPT1fO+/v7IyIiAhs3boS3tzd2796NvXv3arwAiIiISF96jR5y69Yt7Nu3D6mpqSgqKlJZtnz5cq3X8/wtJurExsaWKRsyZAiGDBmi9XaIiIj0pXOijImJweuvv44WLVrgjz/+QPv27ZGSkgJRFNGpU6eqiJGIiMhgdD71OmvWLEybNg2XL1+GmZkZfvzxR6SlpaFHjx48yiMiojpH50SZmJiIkSNHAgCMjY2Rn58PKysrzJ8/H1988UWlB0hERGRIOidKS0tL5feSzs7O+Ouvv5TL7t27V3mRERER1QA6f0fZtWtXnDx5Eu7u7ggODsbUqVNx+fJl7NmzB127dq2KGImIiAxG50S5fPly5OTkAADmzZuHnJwc7Ny5E25ubjpd8UpERFQb6JwoFy5ciLfffhtAyWlYDm1FRER1mc7fUd69exf9+vWDi4sLpk+fjkuXLlVFXERERDWCzonyp59+Qnp6OmbPno34+Hh06tQJHh4eWLhwIVJSUqogRCIiIsPR6xF2dnZ2eO+99xAbG4ubN29i9OjR+P7779GqVavKjo+IiMigXuhZrzKZDOfOncOZM2eQkpJSZkBlIiKi2k6vRHns2DGMGzcOjo6OGD16NGxsbLB//37cunWrsuMjIiIyKJ2vem3cuDEePHiAfv36YePGjQgJCVE7MDIREVFdoHOinDt3LoYMGYJ69epVQThEREQ1i86Jcty4cVURBxERUY1k0IGbiYiIajomSiIiIg2YKImIiDQwaKI8ceIEQkJC0KhRIwiCgL1792qsHxsbC0EQykwZGRnVEzAREb10DJooc3Nz4e3tja+++kqndklJSUhPT1dODRs2rKIIiYjoZafzVa+VqX///ujfv7/O7Ro2bMjbU4iIqFoYNFHqq0OHDigsLET79u0xd+5cdOvWrdy6hYWFKCwsVM5nZ2cDKHn8nkwmq/JYDaF0v+rq/lUV9pv+2Hf6Yb/pp7L6Tdv2giiK4gttqZIIgoDIyEiEhoaWWycpKQmxsbHw8fFBYWEhvv76a3z//fc4c+YMOnXqpLbN3LlzMW/evDLlERERsLCwqKzwiYiolsnLy8Pw4cORlZUFGxubcuvVqkSpTo8ePeDq6orvv/9e7XJ1R5QuLi64d++exo6pzWQyGaKjoxEYGAgTExNDh1NrsN/0x77TD/tNP5XVb9nZ2XBwcKgwUdbKU6/P6tKlC06ePFnucqlUqvZZtCYmJnX+jfky7GNVYL/pj32nH/abfl6037RtW+vvo0xISICzs7OhwyAiojrKoEeUOTk5uHHjhnI+OTkZCQkJsLe3h6urK2bNmoXbt29j69atAICVK1eiefPm8PDwQEFBAb7++mv88ssvOHLkiKF2gYiI6jiDJspz586hV69eyvkpU6YAAEaNGoUtW7YgPT0dqampyuVFRUWYOnUqbt++DQsLC3h5eeHo0aMq6yAiIqpMBk2UPXv2hKZribZs2aIyP2PGDMyYMaOKoyIiInqq1n9HSUREVJWYKImIiDRgoiQiItKAiZKIiEgDJkoiIiINmCiJiIg0YKIkIiLSgImSiIhIAyZKIiIiDZgoiYiINGCiJCIi0oCJkoiISAMmSiIiIg2YKImIiDRgoiQiItKAiZKIiEgDJkoiIiINmCiJiIg0MGiiPHHiBEJCQtCoUSMIgoC9e/dW2CY2NhadOnWCVCpFq1atsGXLliqPk4iIXl4GTZS5ubnw9vbGV199pVX95ORkDBgwAL169UJCQgImTZqEsWPH4vDhw1UcKRERvayMDbnx/v37o3///lrXX79+PZo3b45ly5YBANzd3XHy5EmsWLECQUFBVRUmERG9xAyaKHV1+vRpBAQEqJQFBQVh0qRJ5bYpLCxEYWGhcj47OxsAIJPJIJPJqiTO6qAQFZDJZShWFEOmkJW8Foshk8tQUFSAR7JHZfbvSuYVyEU5FKICoihChAiFqFBOolgy39ahLRwsHJTtHuY/RHx6vEodZZvn1vFG2zdgLHn6trqYcRG/3/ldpW7ptkVRBACIEOFo6YjBbQerxLvr2i5k5GQo64h4Ul9Ufe3XxA9+TfyU7QqKC7D67Ooy21D3erT3aDSxaQKg5D2Rkp+COcfmQJAIZbf1zDbNjM0w+7XZZeI9n35epd7z7QDAy9ELI71GqrSdEzsHWYVZGtuJEPG259sq+3or+xYW/LpAq31dHrgcduZ2yrYHrh/A9qvb1f4+nn3tauOKpYFLVeKdd2IeEu4kPO0jUcTdu3exYecGSCQSCCjpv9dbv47R3qOV7URRxFt73gIACIKgrKfu9XT/6fBq6KVs+3vm71jx2wqt2m4YsEEl3h+u/YBfUn5R2Vd1P70aemFK1ykqbacfnY7bj2+X3/bJ6zEdxmCg20Blu8zcTLz787sat6cQFXj44CHa3GuDlg4tlW2jbkRh5ZmVqEgDiwbYNnibStn8E/Pxa+qvFbbt36p/mX19fefryJflK+dL34PPm/PaHHRv2l05fyXzCiYenqhSp7y2+9/aDwsTC+X85oTN2Pr71grj9WjggTX91wCA8n/bi/4P17Z9rUqUGRkZcHR0VClzdHREdnY28vPzYW5uXqZNeHg45s2bV6b8yJEjsLCwKFOurVx5LqLvR0MuylEsFkMhKlAsFkMuyiGHvOTnk2XvOL+Deib1lG3PZZ/DgbsHym1XOm9nYodFbotUtvtF8hc4k3UGCig0xhdYPxD1ouuplL31+1soUBRUuG8zms2Afz1/5fy1nGv4+MbHFXcKgO2e22Fu9PT3sC19G3bd2VVhuzYWbSD9W6pSNvfPubied73Ctm85vYWHTg+V8znFOfj0yqdaxWuZYYk2lm2U87cKbmHp6aUaWpSwkFig8+POKmXfpn6LmAcxFbbtVq8bHG45qJRtuLIBD4sfltPiKek9KR7Wf1ovJT8Fm5M2V9gOAHoreqO+aX3l/E+ZP+GH//1QYTtXM1f0lvVWKdt/Yz8u5VwqWzlbdVbySIKGtxsq5xWiApFJkVrF27agLW7Z3FLOn88+j21/b9PQ4qlBwiCV+R23dmD/vf0VtvvD+g+0fdBWpWx34m7cLrxdYduGuQ0huf7026zMokwc/lu7r4WOnjiKJLMk5fyR+0cQmxZbYTsHEwdERUWplMUkx+B01ukK25rmmJbZ19jkWK3+RxyJO4KcqznK+T9y/8CvaRUnZwA4eOigyv+ImPQYxN2Jq7DdvQf3ECWq7mt0dLRW2yxPXl6eVvVqVaLUx6xZszBlytNPTdnZ2XBxcUHfvn1hY2Oj93pTs1Ix4qsRWtVd/uZyuDu4K+fTL6bj4t8XK2xnIjVBcHCwStl3P34HRZbmJAmUfHIPDAyEiYmJssz4mjFQVHG8HTp2QLD70+3a3bIDblTcDgD69u0La6m1cj7+RDxwp+J2dnZ2ZfZ1YeZCQIv3sZubG4Jfe9o2qyALuKJdvP7+/nil8SsASj5dnvzhpFbtTEzK/m4i90cCDypu6+zsXKat+Q1zPMypOFF6eXkh2Ptp2yuZV4AkDQ2e0btPbzS2bqycTzqTBPyv4nZWVlZl4l2zfQ2QU06DZ7Rs0RLBfZ62VYgKQE1+VeeVV15BQPOnZ5Akf0mAvytuJ0AoE29MdAxwr+K2Dg0cyrS1SrMCCstp8Az3du4I7vK0bWpWKnCt4nZAyfvQ08lTOZ9+MR1Iq7idubm52v8RyKq4rYurS5m22v6P6NixY9n/ERV/pgVQ9n/E2eNndf4fIZPJEB0dXeZ/nK5KzzBWpFYlSicnJ9y5o9qjd+7cgY2NjdqjSQCQSqWQSqVlyk1MTF6og82l6renlgQq25KalI1HIkhgIjGBscQYJkYmMJGYwN7CvkyMbRzawOexD0wkJjAxelL/uddGghHqZ9cvs49jOoxBobwQEqHk1JhEkCgnQXg6365hO5V2ze2b47Men2lsU7pOSzNLmBg/bTuo7SC42Lqo1BcgKE+RCULJKbMGFg3K7OuC3gtwP+++ss6z9Z993a6Bary2ElvsHba3pF4Fp+g8HD1U2npYeSAqLArGRsblbhdASV8/F+/H3T/G2M5jVeo9304QBNibl/29Ro2Igkwh09gOAFxsXFTauju648r7V7Ta1yb1mqicFv8/n//D0PZDVfZRXXsTo7J/Kz8M+UEl3uLiYhw9ehR9+vSBsUnJNkRRhIWJhUpbURRxa/ItrU4VO1o5qrTt3aI3/pzwZ4VtRYhl4v3otY8wrvM4lfedup+WppZl2saMjEGxorjCttZSa5W2zeyb4cGMBxrbFcuKcejwoTLvw7E+YzGq4yhURBAElb83ANgxZEfJB5IKGAlGMDFSbXt3+t2y23jyO36WscQYRhIj5fyrzV5F4adafJoAYCIxUb6fAWB+7/mY17vsWT91nn3/Ai/+f1zbtoJY+i4zMEEQEBkZidDQ0HLrzJw5E1FRUbh8+bKybPjw4Xjw4AEOHTqk1Xays7Nha2uLrKysFzqiLCguwP4/96tNVKXJrnS+pX1LmBmbKdsWFhcivzhfpZ1EqLwLkGUyGaKiohAcHPxCb6KXDftNf+w7/bDf9FNZ/aZtPjDoEWVOTg5u3Hh6Ti85ORkJCQmwt7eHq6srZs2ahdu3b2Pr1pIvev/1r39hzZo1mDFjBsaMGYNffvkFP/zwAw4cOFDtsZsZm+HNdm/q1VZqLIXUuOxRJRER1TwGvY/y3Llz6NixIzp27AgAmDJlCjp27Ig5c+YAANLT05Gamqqs37x5cxw4cADR0dHw9vbGsmXL8PXXX/PWECIiqjIGPaLs2bMnNJ35VffUnZ49e+LixYovhCEiIqoMfNYrERGRBkyUREREGjBREhERacBESUREpAETJRERkQZMlERERBowURIREWnARElERKQBEyUREZEGTJREREQaMFESERFpwERJRESkARMlERGRBkyUREREGjBR6uvRI0NHQERE1YCJUh/37wMeHsCECUB+vqGjISKiKsREqStRBMaOBf73P+CrrwAfH+D33w0dFRERVREmSn0EBQHm5iWvr10DfH2BlSsBhcKgYRERUeWrEYnyq6++QrNmzWBmZoZXXnkFZ8+eLbfuli1bIAiCymRmZlZ9wQoC8K9/AefPA97eJWVFRcDkyUBwMJCRUX2xEBFRlTN4oty5cyemTJmCzz77DBcuXIC3tzeCgoKQmZlZbhsbGxukp6crp5s3b1ZjxE+4uwNnzgBTpz4tO3wY8PIC9u+v/niIiKhKGDxRLl++HOPGjcM///lPtGvXDuvXr4eFhQW+/fbbctsIggAnJyfl5OjoWI0RP0MqBZYuBY4cAZydS8ru3gVCQnihDxFRHWFsyI0XFRXh/PnzmDVrlrJMIpEgICAAp0+fLrddTk4OmjZtCoVCgU6dOmHhwoXw8PBQW7ewsBCFhYXK+ezsbACATCaDTCarnB3p2RM4fx5G770HSenR5FdfQfzlFxR//33JUWY1Kt2vStu/lwT7TX/sO/2w3/RTWf2mbXtBFEXxhbb0Av73v/+hcePGOHXqFPz8/JTlM2bMwPHjx3HmzJkybU6fPo3r16/Dy8sLWVlZWLp0KU6cOIGrV6+iSZMmZerPnTsX8+bNK1MeEREBCwuLyt0hUUSzw4fh8e23MC4qAgDIjY1xbeRI/D1wICAx+AE8ERE9kZeXh+HDhyMrKws2Njbl1qt1ifJ5MpkM7u7uCAsLw4IFC8osV3dE6eLignv37mnsmBeSmAjjkSMhXLqkLFL07Qv5pk1PT9FWIZlMhujoaAQGBsLExKTKt1dXsN/0x77TD/tNP5XVb9nZ2XBwcKgwURr01KuDgwOMjIxw584dlfI7d+7AyclJq3WYmJigY8eOuHHjhtrlUqkUUqlUbbsqe2N6eZVc6PPJJ8CyZQAAyZEjkHTuDGzeDAwcWDXbfU6V7mMdxn7TH/tOP+w3/bxov2nb1qDnAk1NTdG5c2fExMQoyxQKBWJiYlSOMDWRy+W4fPkynKvhSE0n6i70uXev5EKf8eOBvDzDxkdERFox+JdmU6ZMwaZNm/Ddd98hMTER77//PnJzc/HPf/4TADBy5EiVi33mz5+PI0eO4O+//8aFCxfw9ttv4+bNmxg7dqyhdkGzwMCSJ/e8/vrTsrVrS57o88ypWSIiqpkMeuoVAIYNG4a7d+9izpw5yMjIQIcOHXDo0CHlLR+pqamQPHMRzMOHDzFu3DhkZGTAzs4OnTt3xqlTp9CuXTtD7ULFHByAvXuBjRtLHkyQnw8kJgJdugCLFgETJ/JCHyKiGsrgiRIAJkyYgAkTJqhdFhsbqzK/YsUKrFixohqiqmSCAPzf/wHduwPDhwMJCSVP9JkyBTh0CNiypVou9CEiIt3wMKa6ubsDv/0GTJv2tOzIkZILgH7+2XBxERGRWkyUhiCVAkuWANHRqhf6vP468O9/80IfIqIahInSkAICSi70GTToadm6dbzQh4ioBmGiNDQHByAyEtiw4enQXaUX+qxYwaG7iIgMjImyJhAE4L33gAsXgI4dS8pKL/Tp3x9ITzdsfERELzEmypqkbVvg9OmyF/p4egL79hkuLiKilxgTZU3z7IU+jRqVlN2/X/I95vvv80IfIqJqxkRZU5Ve6BMa+rRs/fqSC30SEgwVFRHRS4eJsiarXx/Ys6fshT6vvAIsX84LfYiIqgETZU1X3oU+U6cC/frxQh8ioirGRFlblF7oM33607LoaF7oQ0RUxZgoaxOpFFi8GDh6lBf6EBFVEybK2qhPH/UX+nTuDFy8aLCwiIjqIiZKPdy4ARw8CJw6BVy5AqSlAdnZ1XxtjboLff74A3jlFUj4RB8iokpTI4bZqm1+/BH46KOy5YIA2NgAtrYl07Ovn5/KW2ZjAxgZaRlI6YU+3bsDI0aUXPAjk8Fo5kz0bNYMRt9+C1hbA5aWTycLC9X58qbSeloHQ0RUNzFR6iErS325KJYsK2+5tqysdE2ybWG79jfYfrMcNpuWwRaPYJuSAqSkvFggQMn3otokVG0Tr6VlyRGwqSlgYlIyGRuXJH0iohqIiVIPAQEl/+dLk2J29tPXz04FBfqtPyenZPrf/3RpZQJg5pMJMEERpCiE6ZOfpZPO84VFkBYWQvqgvPr3IMVtrdZniiKUmw6NjZ8mThMT1URa3lTJdQRBgOOlSxAEoeQDgrFxyRG1kZHur8tbxg8ERLUOE6UeevcumSpSVFR+Ei2v/PllOTn6xSiDKWQw1a9xFTJB0ZOEKUKACAkUJa+LRUiKFRDynyt/5rW6Mm1e61Y3ABIoYAQ5jFD05Kf8mTK5xrKK6ypgJBFhJBEhkYgwkkA5/7T8mXwrEWFkBEiMACMJIBFECBKh5KcASCQluVcQREgEqJQpf5ap++S1BOW3eXa5RFBdZiQ8Xa9EgEQCiBDhmJ6B9N2nITExhShIIEqMVKdny8osl0AUnl32ZL603vNtBQkUzy9//vWTCUZGEIRnPrcYA0bGEpXPNEbGwtPPMyYSlbJnfxqbCDAyFkr6oLTznu/c5ydNy+RyCDIZIJOV/IGoq0MGVyMS5VdffYUlS5YgIyMD3t7e+PLLL9GlS5dy6+/atQuzZ89GSkoK3Nzc8MUXXyA4OLgaI9aOqWnJKFoODvqvQy5/mji1Ta6PHimQnp4NMzNbFBUJKCwECgtLEnfpa0Nd61NTE3i1UjyZqFaTqPmQZAQ5jFGstry8OgIaIxzajj8rqPx4vlxUJlZ19QS1LwFBfb1nqz9TJAhiOWUCBEEsaVa6SqHscnX1BEEEBEG5ydIPfyV1y64HEFFUZIfM109j3Obuz3dGpTN4oty5cyemTJmC9evX45VXXsHKlSsRFBSEpKQkNGzYsEz9U6dOISwsDOHh4Rg4cCAiIiIQGhqKCxcuoH379gbYg6plZATY2ZVM2pLJ5IiKOo7g4GCYmJiorVNcrJo4n0+k2szr06aoqGQSxaeTQqH5tWqZWPJa8cxrEVDIAVEUy9YXBdUyCFAo+CmdXpyi5PwAZIbYuKhjeR3lfulYtWxHEEXRoF37yiuvwNfXF2vWrAEAKBQKuLi44IMPPsBHai4tHTZsGHJzc7F//35lWdeuXdGhQwesX7++wu1lZ2fD1tYWWVlZsLGxqbwdqUFkMhmioqI0Jkoqm4QLC2WIijqEvn37QSIxgVwO5aRQQGX+Rcv0ba/yAUIuQhTFkp/PlinEkjoKseTDhALPzKNsm2friOKTDx1ly5++fvohpWS9QLFcgYf378Ghvj0kgqA8pS08c3pbEMWy5aKaeVGhpo5CuUwiKErqPFeu+vrJzyeTQgTkCgmKFRLIFULJJAqQKyTKn8plogC5KFFd9uy8KFFZXiwaPTOvOhWrzJechH+6zAgi79B7IZM6HcOK8730bq9tPjDoEWVRURHOnz+PWbNmKcskEgkCAgJw+vRptW1Onz6NKVOmqJQFBQVh7969ausXFhaisLBQOZ/15JLUBw8eQCYzyGfBKieTyZCXl4f79+8zUepAJpNBLs9BQcHTfiv9bos0k8lkOHbsHHr16vXMe04A8LLfXiQCkD+Zyv6/KSqS4dixY8/1W8UM8dVl6QfK0tfqyjQuV5ScChIVJR/moFAAChGivOTTlygCUChKPsA9+URWWk8U8bSeXAFZcTHiz55F96DuuH//vt779Pjx4ydxaj5eNOi/gHv37kEul8PR0VGl3NHREX/88YfaNhkZGWrrZ2RkqK0fHh6OefPmlSlv3ry5nlETEVGN8EnlrObx48ewtbUtd3md/6w8a9YslSNQhUKBBw8eoH79+iW3AdRB2dnZcHFxQVpaWp09vVwV2G/6Y9/ph/2mn8rqN1EU8fjxYzQqfXZ2OQyaKB0cHGBkZIQ7d+6olN+5cwdOTk5q2zg5OelUXyqVQiqVqpTVq1dP/6BrERsbG/7x6YH9pj/2nX7Yb/qpjH7TdCRZyqDfJJuamqJz586IiYlRlikUCsTExMDPz09tGz8/P5X6ABAdHV1ufSIiohdh8FOvU6ZMwahRo+Dj44MuXbpg5cqVyM3NxT//+U8AwMiRI9G4cWOEh4cDACZOnIgePXpg2bJlGDBgAHbs2IFz585h48aNhtwNIiKqowyeKIcNG4a7d+9izpw5yMjIQIcOHXDo0CHlBTupqamQSJ4e+Pr7+yMiIgKffvopPv74Y7i5uWHv3r118h5KfUmlUnz22WdlTjmTZuw3/bHv9MN+009195vB76MkIiKqyXi3KxERkQZMlERERBowURIREWnARElERKQBE2UdEh4eDl9fX1hbW6Nhw4YIDQ1FUlKSocOqdRYtWgRBEDBp0iRDh1Lj3b59G2+//Tbq168Pc3NzeHp64ty5c4YOq0aTy+WYPXs2mjdvDnNzc7Rs2RILFiyo8HmjL6MTJ04gJCQEjRo1giAIZZ7pLYoi5syZA2dnZ5ibmyMgIADXr1+v9DiYKOuQ48ePY/z48fjtt98QHR0NmUyGvn37Ijc319Ch1Rrx8fHYsGEDvLy8DB1Kjffw4UN069YNJiYmOHjwIK5du4Zly5bBTpcx4V5CX3zxBdatW4c1a9YgMTERX3zxBRYvXowvv/zS0KHVOLm5ufD29sZXX32ldvnixYuxevVqrF+/HmfOnIGlpSWCgoJQUFBQuYGIVGdlZmaKAMTjx48bOpRa4fHjx6Kbm5sYHR0t9ujRQ5w4caKhQ6rRZs6cKb766quGDqPWGTBggDhmzBiVsjfeeEMcMWKEgSKqHQCIkZGRynmFQiE6OTmJS5YsUZY9evRIlEql4vbt2yt12zyirMNKhxSzt7c3cCS1w/jx4zFgwAAEBAQYOpRaYd++ffDx8cGQIUPQsGFDdOzYEZs2bTJ0WDWev78/YmJi8OeffwIALl26hJMnT6J///4Gjqx2SU5ORkZGhsrfq62tLV555ZVyh2nUl8GfzENVQ6FQYNKkSejWrRufWqSFHTt24MKFC4iPjzd0KLXG33//jXXr1mHKlCn4+OOPER8fjw8//BCmpqYYNWqUocOrsT766CNkZ2ejbdu2MDIyglwux+eff44RI0YYOrRapXRoRV2GXdQXE2UdNX78eFy5cgUnT540dCg1XlpaGiZOnIjo6GiYmZkZOpxaQ6FQwMfHBwsXLgQAdOzYEVeuXMH69euZKDX44YcfsG3bNkRERMDDwwMJCQmYNGkSGjVqxH6roXjqtQ6aMGEC9u/fj2PHjqFJkyaGDqfGO3/+PDIzM9GpUycYGxvD2NgYx48fx+rVq2FsbAy5XG7oEGskZ2dntGvXTqXM3d0dqampBoqodpg+fTo++ugjvPXWW/D09MQ777yDyZMnKwd+IO2UDq2oy7CL+mKirENEUcSECRMQGRmJX375Bc2bNzd0SLVCnz59cPnyZSQkJCgnHx8fjBgxAgkJCTAyMjJ0iDVSt27dytx+9Oeff6Jp06YGiqh2yMvLUxnoAQCMjIygUCgMFFHt1Lx5czg5OakMu5idnY0zZ85U+rCLPPVah4wfPx4RERH46aefYG1trTxPb2trC3NzcwNHV3NZW1uX+R7X0tIS9evX5/e7GkyePBn+/v5YuHAhhg4dirNnz2Ljxo0c8q4CISEh+Pzzz+Hq6goPDw9cvHgRy5cvx5gxYwwdWo2Tk5ODGzduKOeTk5ORkJAAe3t7uLq6YtKkSfjPf/4DNzc3NG/eHLNnz0ajRo0QGhpauYFU6jW0ZFAA1E6bN282dGi1Dm8P0c7PP/8stm/fXpRKpWLbtm3FjRs3GjqkGi87O1ucOHGi6OrqKpqZmYktWrQQP/nkE7GwsNDQodU4x44dU/s/bdSoUaIoltwiMnv2bNHR0VGUSqVinz59xKSkpEqPg8NsERERacDvKImIiDRgoiQiItKAiZKIiEgDJkoiIiINmCiJiIg0YKIkIiLSgImSiIhIAyZKIiIiDZgoiUhrsbGxEAQBjx49MnQoRNWGiZKIiEgDJkoiIiINmCiJahGFQoHw8HA0b94c5ubm8Pb2xu7duwE8PS164MABeHl5wczMDF27dsWVK1dU1vHjjz/Cw8MDUqkUzZo1w7Jly1SWFxYWYubMmXBxcYFUKkWrVq3wzTffqNQ5f/48fHx8YGFhAX9//zLDbRHVJUyURLVIeHg4tm7divXr1+Pq1auYPHky3n77bRw/flxZZ/r06Vi2bBni4+PRoEEDhISEQCaTAShJcEOHDsVbb72Fy5cvY+7cuZg9eza2bNmibD9y5Ehs374dq1evRmJiIjZs2AArKyuVOD755BMsW7YM586dg7GxMYeIorqt0scjIaIqUVBQIFpYWIinTp1SKX/33XfFsLAw5ZBEO3bsUC67f/++aG5uLu7cuVMURVEcPny4GBgYqNJ++vTpYrt27URRFMWkpCQRgBgdHa02htJtHD16VFl24MABEYCYn59fKftJVNPwiJKolrhx4wby8vIQGBgIKysr5bR161b89ddfynrPju5ub2+PNm3aIDExEQCQmJiIbt26qay3W7duuH79OuRyORISEmBkZIQePXpojMXLy0v52tnZGQCQmZn5wvtIVBMZGzoAItJOTk4OAODAgQNo3LixyjKpVKqSLPVlbm6uVT0TExPla0EQAJR8f0pUF/GIkqiWaNeuHaRSKVJTU9GqVSuVycXFRVnvt99+U75++PAh/vzzT7i7uwMA3N3dERcXp7LeuLg4tG7dGkZGRvD09IRCoVD5zpPoZccjSqJawtraGtOmTcPkyZOhUCjw6quvIisrC3FxcbCxsUHTpk0BAPPnz0f9+vXh6OiITz75BA4ODggNDQUATJ06Fb6+vliwYAGGDRuG06dPY82aNVi7di0AoFmzZhg1ahTGjBmD1atXw9vbGzdv3kRmZiaGDh1qqF0nMixDf0lKRNpTKBTiypUrxTZt2ogmJiZigwYNxKCgIPH48ePKC21+/vln0cPDQzQ1NRW7dOkiXrp0SWUdu3fvFtu1ayeamJiIrq6u4pIlS1SW5+fni5MnTxadnZ1FU1NTsVWrVuK3334riuLTi3kePnyorH/x4kURgJicnFzVu09kEIIoiqKBczURVYLY2Fj06tULDx8+RL169QwdDlGdwe8oiYiINGCiJCIi0oCnXomIiDTgESUREZEGTJREREQaMFESERFpwERJRESkARMlERGRBkyUREREGjBREhERacBESUREpMH/A+M2nj4QARDoAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 500x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plot the curve of loss and acc\n",
    "print(log)\n",
    "x=np.arange(epoch)\n",
    "x=x+1\n",
    "plt.figure(figsize=(5,3))\n",
    "plt.plot(x,log[:,0],linestyle='-',color='r',label='train loss',linewidth=2)\n",
    "plt.plot(x,log[:,1],linestyle='-',color='b',label='val loss',linewidth=2)\n",
    "plt.plot(x,log[:,2],linestyle='--',color='g',label='val acc',linewidth=2)\n",
    "plt.title('Training loss and accuracy',fontsize=10)\n",
    "plt.xlabel('epoch',fontsize=10)\n",
    "plt.ylabel('value',fontsize=10)\n",
    "plt.legend(fontsize=10)\n",
    "plt.ylim(0,4)\n",
    "plt.grid()\n",
    "plt.savefig('../Results/training_loss_record.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part7.保存模型的状态字典"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 等两次完整的迭代进行完毕后，保存训练好的模型及其参数\n",
    "save_info = {  # 保存的信息: 1.迭代步数 2.优化器的状态字典 3.模型的状态字典\n",
    "    \"iter_num\": iter_num, \"optimizer\": optimizer.state_dict(), \"model\": model.state_dict()\n",
    "}\n",
    "save_path = \"../Results/model.pth\"  # 将模型存储的位置在当前根目录的文件夹中\n",
    "torch.save(save_info, save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part8.模型测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "# print(data_test_loader.batch_size)\n",
    "predicted_list = []\n",
    "model.eval() # 切换模型为测试状态(没加drop_out层，因此这句话可以随便注释掉)\n",
    "with torch.no_grad():  # 测试集不用算梯度\n",
    "    for batch_idx, testdata in enumerate(data_test_loader):\n",
    "        data = testdata['data'].to(device)\n",
    "        outputs = model(data)\n",
    "        _, predicted = torch.max(outputs.to('cpu').data, dim=1)  # dim = 1 列是第0个维度，行是第1个维度，沿着行(第1个维度)去找1.最大值和2.最大值的下标\n",
    "        # print(predicted.shape)\n",
    "        predicted = predicted.tolist()\n",
    "        if batch_idx <= math.floor(len(test_dataset)/data_test_loader.batch_size) -1:\n",
    "            for i in range(data.size(0)):\n",
    "                predicted_list.append(predicted[i])\n",
    "        else:\n",
    "            for i in range(len(test_dataset) - batch_idx * data_test_loader.batch_size):\n",
    "                predicted_list.append(predicted[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part9.将模型测试结果存入.txt和.csv中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将预测结果写入Predict.txt文件中\n",
    "count = 0\n",
    "with open('../Results/Predict.txt', 'w') as file:\n",
    "    # 遍历列表中的元素并将其写入文件\n",
    "    for item in predicted_list:\n",
    "        file.write(str(count) + '.npy'+ ' ')\n",
    "        file.write(str(item) + '\\n')\n",
    "        count = count + 1\n",
    "# 将预测结果写入Predict.csv文件中\n",
    "with open('../Results/Predict.csv', 'w') as file:\n",
    "    file.write(\"predicted class label\" + '\\n')\n",
    "    # 遍历列表中的元素并将其写入文件\n",
    "    for item in predicted_list:\n",
    "        file.write(str(item) + '\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('AAI_Project')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cbb629c1a6c6048ec23970cc4b70d31a5f99af71a13cd362ad1e3e50eaecb040"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
